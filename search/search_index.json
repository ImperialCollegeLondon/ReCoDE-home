{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"licence/","title":"Licence","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"exemplars/aiforpatents/","title":"Binary Classification of Patent Texts","text":"Redirecting to exemplar docs...","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#binary-classification-of-patent-text-using-natural-language-processing","title":"Binary Classification of Patent Text Using Natural Language Processing","text":"","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#description","title":"Description","text":"<p>There were 193,460 European patent applications filed at the European Patent Office in 2022.</p> <p>The EPO, and several other agencies are really interested in trends associated with the filings of patents to specific areas such as \u2018Green Plastics\u2019 (e.g., plastics that can be recycled, or that are made from biodegradable materials).</p> <p>Typically, to identify whether a patent is related to a certain topic or not, a person would have to manually read through a patent application and assign classification labels to it based on their opinions. Patent applications can be hunderds of pages long, and with the sheer amount of applications that the EPO receive annually, it's easy to see why patent classification is a tedious task!</p> <p>Hence, there is a need for quick and robust methods of accurately classifying the plethora of patents being submitted to the EPO to highlight any trends in \u2018Green Plastics\u2019 filings, or filings in any other areas of interest (e.g., renewable energies, artificial intelligence, augmented reality, drug discovery)</p> <p>By employing machine learning, in the form of Natural Language Processing algorithms, the cost, and likelihood of misclassification of patents, in any technical area, can be significantly reduced, while speeding up the process.</p> <p>To address the challenge of classifying patents, the EPO held its first ever Codefest, where it challenged entrants to develop creative and reliable artificial intelligence (AI) models for automating the identification of patents related to green plastics.</p> <p>To enable contestants to develop their models, the EPO provided access to its extensive dataset of patents and patent classifications. From this, we created a  smaller, binary classification dataset, with half of the entries being related to 'Green Plastics' patents, and the other half being related to other patent areas.</p>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#learning-outcomes","title":"Learning Outcomes","text":"","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#what-youll-learn-from-each-notebook","title":"What you'll learn from each Notebook:","text":"","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#introduction-start-with-this","title":"Introduction (Start with this)","text":"<ul> <li>Why we need to classify patents.</li> <li>What is Tensorflow?</li> <li>Loading datasets into your workspace.</li> <li>What Tokenisation, Vectorisation and Word Embeddings are in the context of NLP.</li> <li>Methods to analyse and understand a dataset.</li> <li>Training a model using the Term-Frequency - Inverse Document Frequency (TF-IDF) vectorisation technique with a Multinomial Bayes algorithm.</li> </ul>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#multi-layer-perceptron-complete-this-after-the-introduction-notebook","title":"Multi-Layer Perceptron (Complete this after the Introduction Notebook)","text":"<ul> <li>What a MultiLayer Perceptron is, and how they can be used for text classification.</li> <li>How to train a Multilayer Perceptron using Tensorflow's Keras.</li> <li>Making predictions using a Multilayer Perceptron.</li> <li>What are hyperparameters and how do they affect the training and performance of machine learning models.</li> <li>Optimise a model's training pipeline using Callbacks</li> <li>Visualise a model and plotting training loss curves.</li> <li>Visualising the structure of a compiled model.</li> <li>Evaluating the performance of a MultiLayer Perceptron.</li> </ul>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#long-short-term-memory-networks-lstms-complete-after-intro-and-multi-layer-perceptron-notebooks","title":"Long Short Term Memory Networks (LSTMs) (Complete after Intro and Multi-Layer Perceptron Notebooks)","text":"<ul> <li>What a LSTM is, and how they can be used for text classification.</li> <li>How to train a LSTM using Tensorflow's Keras.</li> <li>Evaluating the performance of the LSTM.</li> </ul>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#one-dimensional-convolutional-neural-networks-1d-cnn-complete-after-intro-and-multi-layer-perceptron-notebooks","title":"One-Dimensional Convolutional Neural Networks (1D-CNN) (Complete after Intro and Multi-Layer Perceptron Notebooks)","text":"<ul> <li>What a 1D-CNN is, and how they can be used for text classification.</li> <li>How to train a 1D-CNN using Tensorflow's Keras.</li> <li>What a pooling layer is?</li> <li>Evaluating the performance of the 1D-CNN.</li> </ul>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#transformers-complete-after-intro-multi-layer-perceptron-and-lstm-notebooks","title":"Transformers (Complete after Intro, Multi-Layer Perceptron and LSTM Notebooks)","text":"<ul> <li>What a Transformer is, and how they can be used for text classification.</li> <li>How to train a Transformer using Tensorflow's Keras and Object Oriented Programming.</li> <li>What is attention?</li> <li>What is a softmax layer?</li> <li>Evaluating the performance of the Transformer.</li> </ul> Task Time Reading 25 hours Practising 20 hours","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#requirements","title":"Requirements","text":"<p>It would help a lot if you went through the following Graduate School courses before going through this exemplar:</p> <p>Data Exploration and Visualisation</p> <p>Data Processing with Python Pandas</p> <p>Plotting in Python with Matplotlib</p> <p>Introduction to Machine Learning</p> <p>Mathematics for Machine Learning Specialisation (Coursera)</p>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#academic","title":"Academic","text":"<ul> <li>Access to Google Colaboratory</li> <li>Basic Math (matrices, averages)</li> <li>Programming skills (python, pandas, numpy, tensorflow)</li> <li>Machine learning theory (at level of intro to machine learning course)</li> </ul>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#getting-started","title":"Getting Started","text":"<p>Just open up the 'Introduction_and_Data_Handling' notebook and click on the blue 'Open in Colab' button to get started.</p>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 Datasets\n|   \u251c\u2500\u2500 GreenPatents_Dataset.csv\n|   \u251c\u2500\u2500 NotGreenPatents_Dataset.csv\n\u251c\u2500\u2500 docs\n|   \u251c\u2500\u2500 1_Introduction_and_Data_Handling.ipynb\n|   \u251c\u2500\u2500 2_Multilayer_Perceptron_Classification.ipynb\n|   \u251c\u2500\u2500 3_LSTM_Classification.ipynb\n|   \u251c\u2500\u2500 4_Transfomer_Classification.ipynb\n|   \u251c\u2500\u2500 5_Convolutional_1D_Network_Classification.ipynb\n</code></pre>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/aiforpatents/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["NumPy","Tensorflow","Scikit Learn","Object-Oriented Programming","Data Analysis","Machine Learning","NLTK","Natural Language Processing","Patents","Statistics"]},{"location":"exemplars/bleneighbour/","title":"BLE Neighbour Discovery Protocol","text":"Redirecting to exemplar docs...","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#a-ble-bluetooth-low-energy-neighbour-discovery-protocol-on-nrf52","title":"A BLE (Bluetooth Low Energy) Neighbour Discovery Protocol on nRF52","text":"<p>This exemplar demonstrates the implementation of a BLE neighbor discovery protocol on the nRF52 platform using nRF Connect for VS Code. Each device alternates between advertising and scanning according to a scheduled time pattern, enabling unidirectional neighbor discovery. Once a peer is discovered, the device initiates a connection and uses a custom BLE service to exchange data. </p> <p>Although Nordic provides a BLE tutorial course, the official examples typically demonstrate a device in the Peripheral role (GAP, Generic Access Profile) or Server role (GATT, Generic Attribute Profile) interacting with a mobile app. This exemplar goes further by walking through the Central and Client as well, enabling your devices to interact directly with each other.</p> <p>This exemplar was developed at Imperial College London by Sabrina Wang in collaboration with Jay DesLauriers from Research Software Engineering and Dan Cummins from Research Computing &amp; Data Science at the Early Career Researcher Institute.</p>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#learning-outcomes","title":"Learning Outcomes \ud83c\udf93","text":"<p>After completing this exemplar, students will:</p> <ul> <li>Understand basic BLE stack concepts.</li> <li>Implement BLE advertising, scanning, and connection.</li> <li>Build a simple neighbor discovery and data exchange application using BLE services.</li> </ul>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#target-audience","title":"Target Audience \ud83c\udfaf","text":"<ul> <li> <p>Graduate and undergraduate students new to BLE technology who need practical experience for their research or projects.</p> </li> <li> <p>Students seeking a hands-on introduction to BLE neighbor discovery protocols and GATT services.</p> </li> <li> <p>Learners interested in wireless communication and low-power device design using nRF52 platforms.</p> </li> <li> <p>Anyone aiming to build foundational BLE skills for academic coursework or prototyping.</p> </li> </ul>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#prerequisites","title":"Prerequisites \u2705","text":"","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#academic","title":"Academic \ud83d\udcda","text":"<ul> <li> <p>Basic knowledge of C programming (variables, functions, control structures).</p> </li> <li> <p>No prior experience with BLE is required \u2014 this exemplar includes guidance and references to official BLE tutorials to support beginners.</p> </li> </ul>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#system","title":"System \ud83d\udcbb","text":"<ul> <li> <p>A Nordic BLE development board (e.g. nRF52832 DK or nRF52840 DK).</p> </li> <li> <p>A USB cable for programming and serial communication.</p> </li> <li> <p>A development environment set up by following Lesson 1 of Nordic's official nRF Connect SDK Fundamentals course:</p> </li> </ul> <p>\ud83d\udc49 https://academy.nordicsemi.com/courses/bluetooth-low-energy-fundamentals/</p>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#getting-started","title":"Getting Started \ud83d\ude80","text":"<ol> <li> <p>Before diving into this exemplar, it\u2019s important to ensure your development environment is properly set up. We highly recommend starting with Lesson 1, Exercise 1 from Nordic Semiconductor\u2019s official nRF Connect SDK Fundamentals tutorial. This exercise will guide you step-by-step through installing the essential development tools and configuring your environment for success.</p> </li> <li> <p>Once you\u2019re comfortable with that, take a moment to complete Exercise 2. It offers practical experience in building and flashing applications onto your development board using the nRF Connect SDK. If you\u2019re new to Nordic\u2019s workflow, this exercise will be especially valuable in helping you become familiar with compiling firmware and programming your device smoothly.</p> </li> <li> <p>The Notebook: demo notebook walks you through the beginner-level example in the <code>demo</code> folder step by step:</p> <ul> <li>Learn the theory: It first refers to the What is BLEnd? Theoretical Foundations and BLE Advertising and Scanning: What You Need to Know documents in the <code>docs</code> folder to introduce the basics of BLE and the BLEnd protocol.   </li> <li>Understand the code: Next, it uses the How to Use a Timer and Introduction to GAP. document to explain how the example code interacts with the official BLE API (Application Programming Interface).   </li> <li>See the results: Finally, it presents the observed output so you can compare your own results with the expected behavior. In this stage, you should see your devices scanning and advertising, allowing them to discover nearby devices.  </li> </ul> </li> <li> <p>The <code>demo_connect</code> example is designed as a challenge exercise, encouraging more self-directed learning. It builds on the basics from the <code>demo</code> example and introduces additional concepts, such as creating and using a customized GATT service after establishing a connection. The Notebook: demo_connect notebook provides learning resources, service explanation and observed results.</p> </li> </ol>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#project-structure","title":"Project Structure \ud83d\uddc2\ufe0f","text":"<p>Overview of code organisation and structure.</p> <pre><code>.\n\u251c\u2500\u2500 demo\n\u2502 \u251c\u2500\u2500 src\n\u2502 \u2502 \u251c\u2500\u2500 advertiser_scanner.c\n\u2502 \u2502 \u251c\u2500\u2500 advertiser_scanner.h\n\u2502 \u2502 \u251c\u2500\u2500 blend.c\n\u2502 \u2502 \u251c\u2500\u2500 blend.h\n\u2502 \u2502 \u251c\u2500\u2500 main.c\n\u2502 \u251c\u2500\u2500 CMakeLists.txt\n\u2502 \u251c\u2500\u2500 prj.conf\n\u251c\u2500\u2500 demo_connect\n\u2502 \u251c\u2500\u2500 src\n\u2502 \u2502 \u251c\u2500\u2500 advertiser_scanner.c\n\u2502 \u2502 \u251c\u2500\u2500 advertiser_scanner.h\n\u2502 \u2502 \u251c\u2500\u2500 blend.c\n\u2502 \u2502 \u251c\u2500\u2500 blend.h\n\u2502 \u2502 \u251c\u2500\u2500 main.c\n\u2502 \u2502 \u251c\u2500\u2500 my_lbs.c\n\u2502 \u2502 \u251c\u2500\u2500 my_lbs.h\n\u2502 \u2502 \u251c\u2500\u2500 my_lbs_client.c\n\u2502 \u2502 \u251c\u2500\u2500 my_lbs_client.h\n\u2502 \u251c\u2500\u2500 CMakeLists.txt\n\u2502 \u251c\u2500\u2500 prj.conf\n\u251c\u2500\u2500 docs\n\u2502 \u251c\u2500\u2500 BLE_Background.md\n\u2502 \u251c\u2500\u2500 BLEnd.md\n\u2502 \u251c\u2500\u2500 introduction_to_GAP.md\n\u2502 \u251c\u2500\u2500 introduction_to_Ktimer.md\n\u251c\u2500\u2500 notebooks\n\u2502 \u251c\u2500\u2500 demo.md\n\u2502 \u251c\u2500\u2500 demo_connect.md\n\u2514\u2500\u2500  README.md\n</code></pre> <p>Code is organised into logical components: - <code>demo</code> for beginner-level code, potentially divided into further modules - <code>demo_connect</code> for challenge code, potentially divided into further modules - <code>docs</code> for documentation - <code>notebooks</code> for tutorials and exercises</p>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#estimated-time","title":"Estimated Time \u23f3","text":"Task Time Reading 4 hours Practising 2 hours","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#additional-resources","title":"Additional Resources \ud83d\udd17","text":"<ul> <li>nRF Connect SDK Fundamentals</li> <li>Bluetooth Low Energy Fundamentals</li> </ul>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/bleneighbour/#licence","title":"Licence \ud83d\udcc4","text":"<p>This project is licensed under the BSD-3-Clause license.</p>","tags":["Wireless Communication","Bluetooth","nRF52","VS Code"]},{"location":"exemplars/combinatorial/","title":"Combinatorial Problems with Reinforcement Learning","text":"Redirecting to exemplar docs...","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#solving-combinatorial-problems-using-reinforcement-learning","title":"Solving Combinatorial Problems using Reinforcement Learning","text":"<p>Combinatorial optimization problems are frequently characterized by being intractable for classical algorithms or prone to suboptimal solutions. Such problems naturally arise in logistics (e.g., vehicle routing), scheduling (e.g., job shop), network design (e.g., flow), finance (e.g., portfolio selection), and beyond. Even minor improvements in these domains can yield substantial benefits in cost, efficiency, or strategy. However, designing heuristics and metaheuristics to tackle their complexity is time-consuming.</p> <p>Reinforcement Learning (RL) has excelled at sequential decision-making tasks in fields ranging from autonomous driving and industrial control to robotics, protein folding, theorem proving, and multiagent games such as chess and go, where it has achieved superhuman performance.</p> <p> </p> <p>In this exemplar, we will focus on learning to use Reinforcement Learning for solving sequential combinatorial problems, where an optimal strategy involves taking specific actions in a sequence while also responding to a probabilistic setting (environment). Notably, Reinforcement Learning is able to learn the state and action space, so it is able to effectively search these spaces for optimal solutions as opposed to exhaustive searches in classical algorithms, without any heuristics that require expert knowledge to correctly derive.</p> <p>We will start by implementing a foundational algorithm, Tabular Q Learning, then learn how to apply it in a pre-supplied environment, involving the famous Monty Hall problem, where we will also explore hyperparameter tuning and visualisation of training. After this, we will learn how you can apply RL to any problem space of interest by creating your own environment, where we will walk through an example implementing an environment from scratch for the seminal News Vendor problem from inventory management.</p> <p>This exemplar was developed at Imperial College London by Omar Adalat in collaboration with Dr. Diego Alonso Alvarez from Research Software Engineering and Dr. Jes\u00fas Urtasun Elizari from Research Computing &amp; Data Science at the Early Career Researcher Institute.</p>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#learning-outcomes","title":"Learning Outcomes \ud83c\udf93","text":"<p>After completing this exemplar, students will:</p> <ul> <li>Explain the core principles of Reinforcement Learning, and be able to identify when and where it is applicable to a problem space, with a particular focus on combinatorial problems for this project.</li> <li>Develop an implementation of a foundational algorithm, Tabular Q Learning, starting from basic principles and concepts.</li> <li>Gain the ability to perform experimental validation of the trained Reinforcement Learning algorithm, visualising the learning over training episodes.</li> <li>Design hyperparameter tuning configurations that can automatically be applied to retrieve the optimal set of hyperparameters.</li> <li>Generalise to non-supplied environments by learning how to create your own Reinforcement Learning environment, allowing you to apply Reinforcement Learning to any problem space that you are interested in.</li> </ul>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#target-audience","title":"Target Audience \ud83c\udfaf","text":"<p>This exemplar is broadly applicable to anyone interested in solving sequential decision problems, which comes up ubiquitously across many domains and industries (e.g. protein synthesis, self-driving cars, planning &amp; scheduling, and strategic games). Specifically, although we focus on sequential combinatorial problems which are a specific flavour of sequential decision problems, the underlying concepts are the same between both.</p> <p>Our exemplar suitable for students, researchers and engineers alike, and academic prerequisite knowledge is not assumed, aside from some confidence in using Python.</p>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#prerequisites","title":"Prerequisites \u2705","text":"","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#academic","title":"Academic \ud83d\udcda","text":"<ul> <li>Basic familiarity with Python &amp; basic programming skills is required to solve the exercises notebooks</li> <li>Some math background in the basics of set theory and probability theory is helpful but not required</li> </ul>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#system","title":"System \ud83d\udcbb","text":"<ul> <li>Astral's uv Python package and project manager</li> <li>An integrated development environment (IDE) for developing and running Python, Visual Studio Code (VS Code) with Python &amp; Jupyter Notebook extensions is the easiest to set up and use. VS Code should automatically prompt you to install the required extensions that you need, but you can refer to here for the Python extension and the Jupyter extension is available for notebook support</li> <li>10 GB of disk space</li> </ul>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#getting-started","title":"Getting Started \ud83d\ude80","text":"<ol> <li>Start by cloning the repository, either using the GitHub interface or Git directly (<code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-Solving-Combinatorial-Problems-using-Reinforcement-Learning</code>).</li> <li>Install Astral's <code>uv</code> if not already installed from the following URL: https://docs.astral.sh/uv/</li> <li>Run the command <code>uv sync</code> in the cloned repository directory. This will install the correct version of Python (scoped to the directory under <code>uv</code>) and gather all dependencies needed.</li> <li>Create a virtual environment under which the Jupyter Notebooks will run under, which will be scoped to the project directory. Simply run <code>uv venv --python 3.12</code>. When running any notebook, use the virtual environment created for Python 3.12 in the current directory's path, VS Code will give you a list selection of virtual environments to run under (you can also switch this in the top right of a notebook as of the time of writing).</li> <li>Ensure that your notebook's working directory is set to the root of the project directory (the top level folder that lists <code>notebooks</code>, <code>README.md</code>, <code>src</code>, etc). If you are using VS Code, this is already specified by <code>.vscode/settings.json</code> of this repository. If you are using a different IDE, be sure to specify this equivalently, it will be evident that this is working if notebooks are able to properly import all local imports.</li> <li>Navigate to the five notebooks in the directory <code>/notebooks/</code> and complete them in order, running the exercises which will be checked against automated tests and checking the solutions if at any time you are stuck!</li> </ol>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#disciplinary-background","title":"Disciplinary Background \ud83d\udd2c","text":"<p>Reinforcement Learning is a powerful learning paradigm in Artificial Intelligence &amp; Computer Science. While Deep Learning and general Machine Learning are very interesting, often the focus is on making a single isolated decision as in the tasks of classification or regression. Reinforcement Learning, which at the state-of-the-art level also uses Deep Learning for effective learning, is important to learn and master for solving more complex tasks that involve sequential decisions.</p> <p>Specifically, as it solves sequential decision problems, it is incredibly useful in an interdisciplinary manner for various problems that arise such as the aforementioned: scheduling, protein synthesis, finance, autonomous vehicles and beyond.</p>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#software-tools","title":"Software Tools \ud83d\udee0\ufe0f","text":"<ul> <li>Python, with version and dependencies managed by Astral's uv</li> <li>Weights &amp; Biases, a web platform for experiment tracking best practices and hyperparameter tuning</li> </ul> <p>A selection of key Python libraries/packages used are listed below: - Gymnasium, allowing to define custom RL environments which conform to a standard interface - Pygame, for visualisation of the environments - Matplotlib, for visualisation of training results by plotting charts and diagrams - Jupyter Notebooks, for literate programming &amp; interactive content</p>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#project-structure","title":"Project Structure \ud83d\uddc2\ufe0f","text":"<p>Overview of code organisation and structure.</p> <pre><code>.\n\u251c\u2500\u2500 notebooks\n\u2502 \u251c\u2500\u2500 1-Intro-to-RL.ipynb\n\u2502 \u251c\u2500\u2500 2-Tabular-Q-Learning.ipynb\n\u2502 \u251c\u2500\u2500 3-Experiments.ipynb\n\u2502 \u251c\u2500\u2500 4-Custom-Envs-News-Vendor.ipynb\n\u2502 \u251c\u2500\u2500 5-Conclusion.ipynb\n\u251c\u2500\u2500 src\n\u2502 \u251c\u2500\u2500 environments\n\u2502 \u251c\u2500\u2500\u2500\u2500\u2500 monty_hall\n\u2502       \u2502          \u2514\u2500\u2500 env.py\n\u2502       \u2502          \u2514\u2500\u2500 state.py\n\u2502       \u2502          \u2514\u2500\u2500 renderer.py\n\u2502       \u2502          \u2514\u2500\u2500 discrete_wrapper.py\n\u2502 \u251c\u2500\u2500\u2500\u2500\u2500 news_vendor\n\u2502       \u2502          \u2514\u2500\u2500 env.py\n\u2502       \u2502          \u2514\u2500\u2500 state.py\n\u2502       \u2502          \u2514\u2500\u2500 renderer.py\n\u2502       \u2502          \u2514\u2500\u2500 discrete_wrapper.py\n\u2502 \u251c\u2500\u2500 rl\n\u2502   \u2502   \u2514\u2500\u2500 common.py\n\u2502   \u2502   \u2514\u2500\u2500 tabular_q_learning.py\n\u251c\u2500\u2500 docs\n\u2514\u2500\u2500 test\n</code></pre> <p>Code is organised into logical components:</p> <ul> <li><code>notebooks</code> for tutorials and exercises<ul> <li><code>solutions</code> contains full solutions to all exercises, implementing all incomplete functions</li> <li><code>extras</code> contains notebooks that allow you to interactively visualise the Monty Hall and News Vendor environments</li> </ul> </li> <li><code>src</code> for core code<ul> <li><code>monty_hall</code> provides the full implementation of the Monty Hall Gymnasium environment. This is something you are expected to import in for Notebook 3. However, later on you can explore this directory in terms of how everything is implemented, for example the discrete state space wrapper, action masking, and visualisation. It may be useful as a reference for any environments you create in the future!</li> <li><code>news_vendor</code> is a full reference/target implementation for Notebook 4.</li> <li><code>rl</code> is a reference implementation for Notebook 2, particularly focused on Tabular Q Learning.</li> </ul> </li> <li><code>docs</code> for documentation</li> <li><code>test</code> for testing scripts</li> </ul>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#best-practice-notes","title":"Best Practice Notes \ud83d\udcdd","text":"<ul> <li>Package (dependency) management and Python version management is provided by <code>uv</code>, which allows a perfectly replicable development environment</li> <li>Reference code is entirely documented and commented using Google's Style of Python Docstrings</li> <li>Experiments are stored and tracked using Weights &amp; Biases, which allows long-term access to results of experiments, accompanied by all necessary information to replicate such experiments such as hyperparameters</li> </ul>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#estimated-time","title":"Estimated Time \u23f3","text":"Task Time Notebook 1. Intro to RL 3 hours Notebook 2. Tabular Q Learning 6 hours Notebook 3. Experiments 2 hours Notebook 4. Custom environment: News Vendor 4 hours Notebook 5. Conclusion 3 hours <p>Total time: 18 hours</p>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#additional-resources","title":"Additional Resources \ud83d\udd17","text":"<ul> <li>For building your Reinforcement Learning knowledge:<ul> <li>Mastering Reinforcement Learning, which is a book accompanied by videos, providing an excellent overview of the various Reinforcement Learning methods out there</li> <li>Reinforcement Learning: An Introduction, a seminal book with its latest edition published in 2018, by Richard S. Sutton and Andrew G. Barto. This book is considered foundational, and both authors heavily contributed to Reinforcement Learning research and helped start the field. However, this book is more on the theoretical side.</li> <li>Spinning Up in Deep RL by OpenAI, which provides a great overview of the state-of-the-art methods (e.g. PPO and actor-critic methods), particularly with deep reinforcement learning.<ul> <li>If you are not familiar with Deep Learning, consider looking at:<ul> <li>Dive into Deep Learning, free online book, with code accompanying each section</li> <li>fast.ai courses, covering advanced deep learning methods from the foundations accompanied by practical implementations</li> </ul> </li> </ul> </li> </ul> </li> <li>Additional combinatorial environments are available at:<ul> <li>Jumanji</li> <li>OR-gym, OR stands for Operations Research </li> </ul> </li> <li>Specifically for attaining better performance in combinatorial RL, you may want to investigate:<ul> <li>More advanced exploration methods, other than greedy-epsilon, starting with Boltzmann</li> <li>Pointer Networks, used by some methods such as AlphaStar</li> <li>Stochastic Q Learning for handling large action spaces</li> <li>Abstraction methods for lowering the complexity of the state and action space</li> </ul> </li> </ul>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/combinatorial/#licence","title":"Licence \ud83d\udcc4","text":"<p>This project is licensed under the BSD-3-Clause license.</p>","tags":["Reinforcement Learning","Machine Learning","Optimisation"]},{"location":"exemplars/cosmicdawn/","title":"CNNs for the Cosmic Dawn","text":"Redirecting to exemplar docs...","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#using-convolutional-neural-networks-to-extract-information-from-the-cosmic-dawn","title":"Using Convolutional Neural Networks to extract information from the Cosmic Dawn","text":"","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#description","title":"Description","text":"<p>This exemplar will explain and demonstrate the steps required to go from image-based data to a finished Convolutional Neural Network (CNN) pipeline, which can be used to extract relevant information from the images. While demonstrating how to solve this machine learning problem, I will also explain how to prototype code in Jupyter notebooks. I will start by explaining how to analyse the statistics of the data to create appropriate training, validation and testing sets; here I will emphasize the importance of uniform parameter spaces. The exemplar will then go through the process of setting up the architecture of the network and how to train it. Once the network is trained I will discuss what possible next steps are, and which is the most appropriate. Finally, I will go through how to convert the code prototyped in Jupyter notebooks into a useable package</p>","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Use a Jupyter Lab notebook to prototype code</li> <li>Use tensorflow to create a CNN to infer parameters from simulated images</li> <li>Convert that prototyped code into a runable script that can then be scaled up to be run on something like the HPC</li> </ul> Task Time Introduction to CNNs 1 hours FirstDawn 3 hours","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#requirements","title":"Requirements","text":"","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#academic","title":"Academic","text":"<ul> <li>Familiarity with Python 3</li> <li>Have used Jupyter Lab before</li> <li>Very little command line knowledge</li> </ul>","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#system","title":"System","text":"<ul> <li>4GB of disk space for datasets</li> <li>Python 3.11 or newer</li> <li>Access to the HPC (optional)</li> </ul>","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#getting-started","title":"Getting Started","text":"<p>To get started, first clone this repo, then change directories into it: <pre><code>  git clone https://github.com/kimeels/ReCode_FirstDawn.git\n  cd ReCode_FirstDawn\n</code></pre></p> <p>If python virtualenv  and Jupyter Lab isn't already installed on your system, install it using:</p> <pre><code>  python3 -m pip install virtualenv\n  python3 -m pip install jupyterlab\n</code></pre> <p>Then create a virtual environment for this exemplar:</p> <pre><code>python3 -m venv venv_recode_firstdawn\n</code></pre> <p>Source into your new virtual environment using:</p> <pre><code>  source venv_recode_firstdawn/bin/activate\n</code></pre> <p>Run this line to install all the necessary packages:</p> <p><pre><code>  pip install -r requirements.txt\n</code></pre> Finally, run these two lines to setup your virtual env with jupyter lab. <pre><code>  pip install ipykernel\n  python -m ipykernel install --user --name=venv_recode_firstdawn\n</code></pre></p>","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 notebooks\n    \u251c\u2500\u2500 figures\n    \u251c\u2500\u2500 Converting to a python script.ipynb\n    \u251c\u2500\u2500 FirstDawn.ipynb\n    \u2514\u2500\u2500 Introduction to CNNs.ipynb\n\u2502   \n\u2514\u2500\u2500 requirements.txt\n</code></pre>","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/cosmicdawn/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["NumPy","Tensorflow","Scikit Learn","Pandas","Convolutional Neural Networks","Machine Learning"]},{"location":"exemplars/crackingtime/","title":"Crackingtime","text":"<ul> <li>Epidemiology</li> <li>Statistics</li> </ul>    Redirecting to exemplar docs..."},{"location":"exemplars/crackingtime/#cracking-times-code-survival-analysis-of-large-datasets","title":"Cracking time's code: Survival analysis of large datasets","text":"<p>Welcome to this ReCoDE project!</p>"},{"location":"exemplars/crackingtime/#description-of-the-project","title":"Description of the project","text":"<p>This is a special type of analysis that takes into consideration when the event occurred rather than if the event occurred. In other words, we are focused on knowing the rate, which is the number of events per unit time. </p> <p>In this exemplar will introduce you to the concept of survival analysis (also known as a time-to-event analysis) using large datasets using R for both unadjusted and adjusted models.</p> <p>A common timescale used in survival analysis is time-to-event, however in large cohort studies data may be left-truncated (participants entering the study at different time points), making the time-scale unsuitable. Instead, age should be considered as the timescale. This is most relevant when exploring age-dependent associations between exposures and outcomes.</p>"},{"location":"exemplars/crackingtime/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Understand the different types of censoring and how to curate your data </li> <li>Conduct univariable and multivariable survival analysis using R</li> <li>Graphically present the findings of a survival analysis</li> <li>Interprete the results from a survival analysis</li> </ul> Task Time Pre-session material 3 hours Data curation 2 hours Analysis 2 hours Visualisation &amp; Interpretation of results 2 hours"},{"location":"exemplars/crackingtime/#what-steps-should-you-follow-when-completing-this-examplar","title":"What steps should you follow when completing this examplar?","text":"<ol> <li>Start by reading the ReCoDE main page.</li> <li>Complete the <code>Introduction</code> section (video lecture, reading materials)</li> <li>Continue with <code>Data Curation</code>(get your data ready for a survival analysis)</li> <li>Conduct a <code>Survival analysis</code> (unadjusted and adjusted)</li> <li>Take your analysis to the next level by attempting the extension task <code>Advanced survival analysis</code> (analysis using different types of death)</li> </ol> <p>Finally I hope you enjoy and learn from this ReCoDE project!</p>"},{"location":"exemplars/crackingtime/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"exemplars/decodingmarketsignals/","title":"Decoding Market Signals","text":"Redirecting to exemplar docs...","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#decoding-market-signals","title":"Decoding Market Signals","text":"","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#leveraging-candlestick-patterns-machine-learning-and-alpha-signals-for-enhanced-trading-strategy-analysis","title":"Leveraging candlestick patterns, machine learning and alpha signals for enhanced trading strategy analysis","text":"","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#description","title":"Description","text":"<p>This project aims to rigorously back-test a trading strategy, focusing on evaluating the informational value of  candlestick patterns. Utilising <code>Python</code>, a pipeline of functions will systematically scan and evaluate  the components of the S&amp;P 500 stock market index for patterns upon which one can base a trading decision.</p> <p>Advanced functionalities of the <code>Pandas</code> library will be employed to load, manipulate and store detailed statistical  data, particularly <code>method chains</code>, <code>multi-indexed data frames</code> and <code>user-defined functions</code> acting on rows and columns of data frames.</p> <p>The project's core involves assessing the predictive capabilities of these trading signals using nuanced binary classification performance  metrics, thereby determining their practical applicability. Additionally, a logistic regression model will be deployed  to explore the intersection of finance and machine learning.  This phase aims to ascertain whether machine learning algorithms can outperform traditional methods in predicting market  movements based on identified signals.</p> <p>This multifaceted project integrates financial analysis, data science, and machine learning,  promising insights with both academic and practical implications.  Its methodologically sound approach, coupled with detailed documentation and learning annotations,  is designed to make it an exemplary contribution to the ReCoDE initiative,  showcasing the transformative potential of research computing and data science in interdisciplinary research.</p>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Setting up a custom computational environment for financial data science.</li> <li>Making a custom technical analysis library written in C++ work with recent Python. </li> <li>Obtaining and pre-processing high-quality financial data.</li> <li>Using Pandas' best-practices like method-chaining, and multi-index data frames for data manipulation  </li> <li>Independently testing and analysing trading actions proposed on a hypothesis.</li> </ul> Task Time Reading 4 hours Practising 6 hours","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#requirements","title":"Requirements","text":"<ul> <li>Foundational knowledge of Python and Pandas.</li> <li>An interest in stock markets and trading signals.</li> <li>An interest in statistical analysis and hypothesis testing.</li> <li>Resilience in troubleshooting and adapting older libraries to work with recent Python versions.</li> <li>Particularly, we will make use of a library called <code>ta-lib</code> that contains a pattern-recognition library detecting candlestick patterns in Open-High-Low-Close <code>(OHCL)</code> data.</li> <li>Familiarity with Jupyter notebooks, type annotations, and automation.</li> </ul>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#academic","title":"Academic","text":"<p>The repository is self-contained. Additional references are provided in the Jupyter notebooks. When we move form a single-stock analysis to the whole investment-universe, the resulting data frames  become too large to work with on a standard machine leading to Kernel crashes. They are themselves not dangerous to the hardware of the computer at all, and one can mitigate this by selecting a subset of the data. If you wish to run the  code on all the data, you need a potent machine, or alternatively execute the code on the HPC facilities.  That does not hinder you from getting started, though.</p>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#system","title":"System","text":"<p>A recent mid-class laptop is sufficient to follow along the code. The more data you wish to analyse, the more RAM it should have. The code was developed on a Linux machine. </p> <p>In this code exemplary, we make use of <code>Python 3.11</code>.  Identifying the candlestick patterns in financial markets data is obtained by using a library that is called <code>ta-lib</code>. It works well for our task, but its Python wrapper is no longer maintained. If you are comfortable with an older version of Python, precisely  <code>Python 3.8</code> or <code>Python 3.9</code>, or just want to get started, it is straightforward to install an older version using <code>pip</code> or <code>conda</code>. </p> <p><code>ta-lib</code> was tested to be installable from <code>pypi</code> on <code>Python 3.8</code> and <code>3.9</code>. If you just want to get started, use <code>Python 3.8</code>. <code>ta-lib</code> can then be installed using <code>pip install TA-Lib</code>.  Alternatively, if you want to make use of the <code>conda</code> package manager, use <code>conda install ta-lib</code> For <code>Python 3.9</code>, the author observed on a Linux operating system, that <code>conda install ta-lib</code> worked straightforward,  whereas <code>pip install ta-lib</code> did not.</p> <p>If you want to make use of later versions of Python such as the environment this project was developed on, precisely  <code>Python v. 3.11</code>, the process is more involved and requires compiling <code>ta-lib's C++</code> files from source.</p> <p>We now encountered two common problems, we frequently face as computer and data scientists: i) Making <code>legacy</code> code run on modern systems, ii) Facing multiple choices of which package manager to use. If you are just interested on getting started, use <code>Python 3.8</code> and skip the following section.</p> <p>For the interested reader, is follows some background information on i) and ii). Issue i) is commonly encountered in practice, especially in larger corporations operating with custom software, whose author's stopped  maintaining their code. There is no silver bullet on working with legacy code and custom problems often need tailor-made solutions. For this project, the author wrote a <code>shell</code> script that is custom-made to set up <code>ta-lib</code> for <code>Python 3.11</code>. Whenever you are not sure whether a solution works and you have reasons to believe your attempt is error-prone, might have side effects, or spoil the operating system, it is advisable  to work from within a virtual environment, for example using <code>Docker</code>, before employing a working solution on the user's machine.  Discussing <code>Docker</code> is beyond the scope of this documentation, however.</p> <p>The <code>shell script</code> can also be directly applied to work on macOS as the latter uses the <code>Z shell</code> by default.  The <code>Z shell</code>, is also known as <code>zsh</code> and is a <code>Unix shell</code> that is built on top of <code>bash</code>. Hence, compatibility  is likely and the script should run without reservation.  For Windows users the <code>shell script</code> can also be modified to work on the <code>Windows shell</code> or <code>PowerShell</code>. The equivalent of a Linux <code>shell script</code> on Windows is a <code>batch script</code> and the commands expressed have to be translated  to make them compatible on Windows.</p> <p>Let us now quickly address issue ii): If your Python environment is set up using miniconda (recommended), see also <code>https://docs.conda.io/projects/miniconda/en/latest/</code>, both, <code>conda</code> and <code>pip</code> are installed by default, and you can make use of both them.  If your Python environment is set up using the source files from <code>https://www.python.org/</code> you might have to install <code>pip</code> separately and cannot use the benefits of conda.</p> <p>What then is the difference between <code>pip</code> and <code>conda</code>? <code>Pip</code> is a package manager specifically designed for Python packages. It primarily focuses on installing and managing Python libraries and packages from the <code>Python Package Index (PyPI)</code>. Pip is used for managing Python dependencies within a Python environment. On the other hand, <code>Conda</code> is a more comprehensive package manager and environment manager. While it can manage Python packages, it is not limited to Python and can handle packages and libraries from various programming languages. Conda is often used to create isolated environments that can include different versions of Python and non-Python dependencies. It can manage both Python packages and system-level packages and is capable of handling complex dependency resolution.</p> <p>Managing and detecting version conflicts of a large Python setup is again a topic on its own. Granted <code>conda</code> is diligent, but slow, the  reader is encouraged to look into promising alternatives like <code>mamba</code>, which is a package manager written in <code>C++</code> and hence more performant  than <code>conda</code>, although less tested.</p> <p>A final note regarding code-formatting. To comply with the PEP-8 style guide for Python code, <code>https://peps.python.org/pep-0008/</code>, we make use of a code-formatter, that automatically spots issues concerning spacing and style. It is applied on code that runs error-free and ensures style consistency. There are several open-source code-formatters out there and arguably the most popular are <code>black</code>, see <code>https://github.com/psf/black</code>, and <code>Ruff</code>, see <code>https://docs.astral.sh/ruff/formatter/</code>. The former is well-tested, however the latter is more performant and has recently gained increasing attention. Hence, we make use of Ruff.</p>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#getting-started","title":"Getting Started","text":"","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#windowsmacos","title":"Windows/MacOS","text":"<p>For Windows and MacOS based machines, you will have to install the <code>ta-lib</code> library separately. Instructions to do so can be found here. Once installed, you can create a virtual environment as suggested below and use <code>pip install ta-lib</code> to install the required Python wrapper, and <code>pip install -r requirements.txt</code> to install the other dependencies. </p>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#linux","title":"Linux","text":"<p>The following code was tested on <code>Ubuntu 22.04.4 LTS</code> using <code>Python 3.11</code>. For other Linux distributions you need to modify the commands  that install software on your system. For example, on <code>Fedora</code>, the default package manager is <code>dnf</code> rather than Ubuntu's <code>apt</code>. Also, you might modify <code>python3.11 -m venv MarketSignals</code> to whatever python version you have. Typically, one can also use the default system Python. The first  line in the below sequence then reads <code>python -m venv MarketSignals</code> rather than <code>python3.11 -m venv MarketSignals</code>. However, on my setup, I opted to install a more recent version of Python , here 3.11. I then have to explicitly call this version to create a virtual environment. Besides, I opted to leave Ubuntu's default system Python  alone. The reason is somewhat more intricate. In short, in contrast to Windows, Ubuntu's bootloader <code>grub</code> critically depends on this system Python. If, for any reason you spoil this system Python, you cannot just delete and re-install it. Should you try to do, your system will not  boot anymore. Hence, I leave the system Python untouched and instead opted for a separate Python version explicitly set for development work.  This <code>dev work Python</code>, I can modify, fine-tune, delete, and re-install without ever even touching the system Python, making it safer to experiment. </p> <p>Execute the following sequences in your Linux terminal to set up your Python environment needed to run the <code>DecodingMarketSignals</code> repository. <pre><code>python3.11 -m venv MarketSignals  # create a virtual environment\nsource MarketSignals/bin/activate  # activate the virtual environment\nsudo apt-get install python3.11-dev  # installs the Python 3.11 development files on your Ubuntu or Debian-based Linux system.\npip install -r requirements.txt  # install dependencies via pip \nchmod +x install-talib.sh  # set execution rights for the shell script to install TA-lib from source\n./install-talib.sh  # run the script installing TA-lib  \njupyter-notebook  # optional: launch an instance of Jupyter notebook and run the examples (assumes you downloaded the CRSP data already).\n</code></pre></p> <p>Start by opening and reading through the Jupyter notebook. All essential steps are separated in respective sub-sections. Once you have an understanding of the overall goal, you can start setting up your Python environment along `ta-lib and  either replicate the results or apply the techniques demonstrated to your own data. The reader is encouraged to apply the methods outlined  on their own data from different markets, for instance, the futures and forex markets. </p>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#project-structure","title":"Project Structure","text":"<pre><code>.\n\n\u251c\u2500\u2500 data\n\n\u2502   \u251c\u2500\u2500 SP500_daily_data_1980_to_2023.csv.gz (to be downloaded by the user via WRDS)\n\n\u2502   \u2514\u2500\u2500 SP500_tickers_one_per_line.txt\n\n\u251c\u2500\u2500 figures\n\n|   \u251c\u2500\u2500 candlestick_anatomy.png\n\n|   \u251c\u2500\u2500 WRDS_overview.png\n\n|   \u251c\u2500\u2500 CRSP1_select_quarterly.png\n\n|   \u251c\u2500\u2500 ...\n\n\u2502   \u2514\u2500\u2500 WRDS_overview.png\n\n\u251c\u2500\u2500 notebooks\n\n|   \u251c\u2500\u2500 BSquant.py\n\n|   \u251c\u2500\u2500 1_obtaining_financial_data.ipynb\n\n|   \u251c\u2500\u2500 2_single_stock_case.ipynb\n\n|   \u251c\u2500\u2500 3_SP500_case.ipynb\n\n\u251c\u2500\u2500 install-talib.sh\n\n\u251c\u2500\u2500 requirements.txt\n\n\u251c\u2500\u2500 mkdocs.yml\n\n\u251c\u2500\u2500 LICENSE.md\n\n\u2514\u2500\u2500 README.md\n</code></pre>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/decodingmarketsignals/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Pandas","Finance","Logistic Regression"]},{"location":"exemplars/deeplearning/","title":"Deep Learning Best Practices","text":"Redirecting to exemplar docs...    <p>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#deep-learning-best-practices","title":"Deep Learning Best Practices","text":"","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#description","title":"Description","text":"<p>This project aims to showcase best practices and tools essential for initiating a successful deep learning project. It will cover the use of configuration files for project settings management, adopting a modular code architecture, and utilizing frameworks like Hydra for efficient configuration. The project will also focus on effective result logging and employing templates for project structuring, aiding in maintainability, scalability, and collaborative ease.</p>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#learning-outcomes","title":"Learning outcomes","text":"<ol> <li>Using Wandb to Log Training Metrics and Images</li> <li> <p>Master the integration of Wandb (Weights &amp; Biases) into your project for comprehensive logging of training metrics and images. This includes setting up Wandb, configuring it for your project, and utilizing its powerful dashboard for real-time monitoring and analysis of model performance.</p> </li> <li> <p>Using Hydra to Manage Configurations</p> </li> <li> <p>Learn to leverage Hydra for advanced configuration management in your projects. Understand how to define, organize, and override configurations dynamically, enabling flexible experimentation and streamlined management of complex projects.</p> </li> <li> <p>Using PyTorch Lightning to Train Models</p> </li> <li> <p>Gain expertise in using PyTorch Lightning to simplify the training of machine learning models. This includes setting up models, training loops, validation, testing, and leveraging PyTorch Lightning's abstractions for cleaner, more maintainable code.</p> </li> <li> <p>Einops for Easy Tensor Manipulation</p> </li> <li> <p>Acquire the skills to use Einops for intuitive and efficient tensor operations, enhancing the readability and scalability of your data manipulation code. Learn to apply Einops for reshaping, repeating, and rearranging tensors in a more understandable way.</p> </li> <li> <p>Learning to Start Training on GPU</p> </li> <li> <p>Understand how to utilize GPUs for training your models. This outcome covers the basics of GPU acceleration, including how to select and allocate GPU resources for your training jobs to improve computational efficiency.</p> </li> <li> <p>Using a Template to Start Project</p> </li> <li>Familiarize yourself with starting new projects using a predefined template, specifically the Minimal Lightning Hydra Template. Learn the benefits of using templates for project initialization, including predefined directory structures, configuration files, and sample code to kickstart your development process.</li> </ol>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#getting-familiar-with-the-libraries","title":"Getting Familiar with the Libraries","text":"<p>To help you become acquainted with the key libraries used in this project, I have prepared learning notebooks that cover the basics of each. These interactive notebooks are an excellent way to get hands-on experience. You can find them in the learning folder. The notebooks are designed to introduce you to the core concepts and functionalities of each library:</p> <ul> <li>Pytorch Lightning: This notebook introduces Pytorch Lightning, a library that simplifies the training process of PyTorch models. It covers basic concepts like creating models, training loops, and leveraging Lightning's built-in functionalities for more efficient training.</li> <li>Hydra: Hydra is a framework for elegantly configuring complex applications. This notebook will guide you through its configuration management capabilities, demonstrating how to streamline your project's settings and parameters.</li> <li>Einops: Einops is a library for tensor operations and manipulation. Learn how to use Einops for more readable and maintainable tensor transformations in this notebook.</li> </ul> <p>For a more comprehensive understanding, I also recommend the following tutorials. They provide in-depth knowledge and are great resources for both beginners and experienced users:</p> <ul> <li>Pytorch Lightning Tutorial: An official guide to starting a new project with Pytorch Lightning, offering step-by-step instructions and best practices.</li> <li>Hydra Documentation: The official introduction to Hydra, covering its core principles and how to integrate it into your applications.</li> <li>Wandb Quickstart: A quickstart guide for Wandb, a tool for experiment tracking, visualization, and comparison. Learn how to integrate Wandb into your machine learning projects.</li> <li>Einops Basics: An introductory tutorial to Einops, focusing on the basics and fundamental concepts of the library.</li> </ul> <p>By exploring these notebooks and tutorials, you will gain a solid foundation in these libraries, which are integral to the project's development.</p>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#starting-a-new-project","title":"Starting a New Project","text":"<p>To main steps to start a new project are described here. This notebook will guide you through the process of initializing a new project and will showcase the best practices and tools used in this project.</p>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#further-learning","title":"Further Learning","text":"<p>This project is designed to provide a comprehensive understanding of best practices in deep learning, incorporating the use of PyTorch, PyTorch Lightning, Hydra, and other essential tools. However, the field of deep learning is vast and constantly evolving. To continue your learning journey, I recommend exploring the following resources:</p> <ul> <li> <p>Deep Learning Tuning Playbook: A detailed guide focused on maximizing the performance of deep learning models. It covers aspects of deep learning training such as pipeline implementation, optimization, and hyperparameter tuning.</p> </li> <li> <p>PyTorch Performance Tuning: This resource provides a set of optimizations and best practices that can accelerate the training and inference of deep learning models in PyTorch. I also recommend exploring the official PyTorch documentation, which is a rich source of tutorials, guides, and examples.</p> </li> <li> <p>Lightning Training Tricks: PyTorch Lightning implements various techniques to aid in training, making the process more efficient and smoother.</p> </li> <li> <p>Scientific Software Best Practices: This exemplar project showcases best practices in developing scientific software, offering insights into structured project management.</p> </li> <li> <p>Further Tools: The deep learning ecosystem includes many other tools and libraries that can enhance your projects. For instance, Fire for automatically generating command line interfaces, DeepSpeed for efficient distributed training and inference, and Optuna for advanced hyperparameter optimization.</p> </li> </ul>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#project-structure","title":"Project Structure","text":"<p>The directory structure of new project looks like this:</p> <pre><code>\u251c\u2500\u2500 .github                   &lt;- Github Actions workflows\n\u2502\n\u251c\u2500\u2500 configs                   &lt;- Hydra configs\n\u2502   \u251c\u2500\u2500 callbacks                &lt;- Callbacks configs\n\u2502   \u251c\u2500\u2500 datamodule               &lt;- Data configs\n\u2502   \u251c\u2500\u2500 debug                    &lt;- Debugging configs\n\u2502   \u251c\u2500\u2500 experiment               &lt;- Experiment configs\n\u2502   \u251c\u2500\u2500 extras                   &lt;- Extra utilities configs\n\u2502   \u251c\u2500\u2500 hparams_search           &lt;- Hyperparameter search configs\n\u2502   \u251c\u2500\u2500 hydra                    &lt;- Hydra configs\n\u2502   \u251c\u2500\u2500 local                    &lt;- Local configs\n\u2502   \u251c\u2500\u2500 logger                   &lt;- Logger configs\n\u2502   \u251c\u2500\u2500 model                    &lt;- Model configs\n\u2502   \u251c\u2500\u2500 paths                    &lt;- Project paths configs\n\u2502   \u251c\u2500\u2500 trainer                  &lt;- Trainer configs\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 eval.yaml             &lt;- Main config for evaluation\n\u2502   \u2514\u2500\u2500 train.yaml            &lt;- Main config for training\n\u2502   \u2514\u2500\u2500 inference.yaml        &lt;- Main config for inference\n\u2502\n\u251c\u2500\u2500 data                   &lt;- Project data\n\u2502\n\u251c\u2500\u2500 logs                   &lt;- Logs generated by hydra and lightning loggers\n\u2502\n\u251c\u2500\u2500 notebooks              &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n\u2502                             the creator's initials, and a short `-` delimited description,\n\u2502                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.\n\u2502\n\u251c\u2500\u2500 scripts                &lt;- Shell scripts\n\u2502\n\u251c\u2500\u2500 src                    &lt;- Source code\n\u2502   \u251c\u2500\u2500 datamodules              &lt;- Data scripts\n\u2502   \u251c\u2500\u2500 models                   &lt;- Model scripts\n\u2502   \u251c\u2500\u2500 utils                    &lt;- Utility scripts\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 eval.py                  &lt;- Run evaluation\n\u2502   \u251c\u2500\u2500 train.py                 &lt;- Run training\n\u2502   \u251c\u2500\u2500 inference.py             &lt;- Run inference\n\u2502\n\u251c\u2500\u2500 tests                  &lt;- Tests of any kind\n\u2502\n\u251c\u2500\u2500 .env.example              &lt;- Example of file for storing private environment variables\n\u251c\u2500\u2500 .gitignore                &lt;- List of files ignored by git\n\u251c\u2500\u2500 .pre-commit-config.yaml   &lt;- Configuration of pre-commit hooks for code formatting\n\u251c\u2500\u2500 .project-root             &lt;- File for inferring the position of project root directory\n\u251c\u2500\u2500 environment.yaml          &lt;- File for installing conda environment\n\u251c\u2500\u2500 Makefile                  &lt;- Makefile with commands like `make train` or `make test`\n\u251c\u2500\u2500 pyproject.toml            &lt;- Configuration options for creating python package\n\u251c\u2500\u2500 requirements.txt          &lt;- File for installing python dependencies\n\u2514\u2500\u2500 README.md\n</code></pre> <p>You will find an extra folder called <code>docs</code> that contains a collection of markdown files with best practices and explanations of how to use the project.</p>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/deeplearning/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["PyTorch","Machine Learning","Best practices"]},{"location":"exemplars/diffusion/","title":"1-Dimensional Neutron Diffusion Solver","text":"Redirecting to exemplar docs...","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#recode-neutron-diffusion-model","title":"ReCoDE - Neutron Diffusion Model","text":"","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#description","title":"Description","text":"<p>This code is part of the Research Computing and Data Science Examples (ReCoDE) project. The code itself is a 1-dimensional neutron diffusion solver written in Fortran in an object-oriented format. The example will focus on features of the code that can be used as a teaching aid to give readers some experience with the concepts such that they can implement them in the exercises or directly in their own codes. An understanding of neutron diffusion and reactor physics is not required for this example, but a discussion of the theory can be found in the bottom section of this readme.</p>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Compiled Codes and Makefiles</li> <li>Compiler Directives</li> <li>Object-Oriented Programming</li> <li>Reading input data from file</li> <li>Generating output files</li> <li>Using ParaView to visualise numerical results</li> <li>Solving mathematical problems</li> <li>Discretisation of a spatial dimension</li> <li>Optimised data storage</li> <li>Using build tools <code>CMake</code> and <code>fpm</code></li> <li>Incorporating external libraries (PETSc)</li> </ul>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#requirements","title":"Requirements","text":"","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#academic","title":"Academic","text":"<p>Entry level researcher with basic knowledge of Fortran syntax. For a Fortran crash course see here</p>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#system","title":"System","text":"Program Version <code>Anaconda</code> &gt;=4.13.0","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#getting-started","title":"Getting Started","text":"<p>You will be needing Anaconda and a shell/bash terminal to run this code. Optionally, you can use Visual Studio Code as your code editor to work on the exemplar.</p>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#creating-the-anaconda-environment","title":"Creating the Anaconda Environment","text":"<p>The first thing required to build the project is to create the Anaconda environment that contains all the necessary tools to run the exemplar. This can be done by running the following command in a terminal:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>You have now created a new Anaconda environment. To activate your Anaconda environment run:</p> <pre><code>conda activate diffusion\n</code></pre> <p>All the following steps take place from a terminal that has the <code>diffusion</code> Anaconda environment activated.</p>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#compiling-the-code","title":"Compiling the Code","text":"<p>As Fortran is a compiled language, the code must first be compiled before it can be executed. To do so, a convenience script has been included to get you started:</p> <pre><code>./install.sh\n</code></pre> <p>or alternatively</p> <pre><code>fpm build\n</code></pre> <p>This runs a few commands, feel free to peek into the <code>install.sh</code> file to see how the code is compiled, although we will be covering building/compiling in detail in sections 3 - Compilation Basics and 7 - Build Tools. The summary of the script is that it calls <code>cmake</code> which configures your project by checking for dependencies and generating the necessary <code>Makefiles</code> for compilation. It then compiles the project using the generated <code>Makefiles</code>, and then it finally installs the program inside a folder called <code>install/bin</code>.</p> <p>To run the installed program, run the following command in a terminal:</p> <pre><code>./install/bin/diffusion\n</code></pre> <p>or alternatively</p> <pre><code>fpm run\n</code></pre> <p>This command tells the executable to run and will generate relevant output files containing the solution to the problem.</p>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#input-options","title":"Input Options","text":"<p>The code is designed such that the user can easily change the problem which the code is attempting to solve. The code uses the input file input.in to read details about the problem, such as the positions of boundaries or materials in the problem. An example of such an input can be seen below:</p> <pre><code>------------------------------------------\nRegions: - Integer number of regions in the problem\n2\n------------------------------------------\nBoundaries: - Real number positions of the boundaries between the regions (one per line)\n0.0\n0.5\n1.0\n------------------------------------------\nNodes: - Integer number of nodes in each region (one per line)\n5\n5\n------------------------------------------\nMaterials: - Fuel, Water or Steel (one per line)\nFuel\nFuel\n------------------------------------------\nBoundary_Conditions: - Zero or Reflective (two parameters - one per line)\nZero\nZero\n------------------------------------------\n</code></pre> <p>For this example problem, we are stating that we have a geometry ranging from x = 0.0 to 1.0, half fuel and half steel with a central boundary at x = 0.5. As seen from the above input, the code needs four different parameters to be described to it.</p>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#anatomy-of-inputin","title":"Anatomy of <code>input.in</code>","text":"<ul> <li>Regions - An integer number of regions that exists within the problem. We have 1 region from 0.0 to 0.5 and another from 0.5 to 1.0, hence we give the code the integer number 2.</li> <li>Boundaries - The positions of the boundaries within the problem. Our first boundary is at the start of our geometry, so we enter the number 0.0. We then have an internal boundary halfway through the problem separating the regions, so we enter the number 0.5. Finally, we have the exterior boundary of our geometry, so we enter the number 1.0. The code will always read one more value here than the number of regions in the problem.</li> <li>Nodes - This describes how refined we want the geometry in each region. For the example we want a quick solve with just enough nodes to see the flux profile. As we need to describe this for each region we enter the value 10 twice. The code will always read the same number of values here as the number of regions in the problem.</li> <li>Materials - This described the materials that are present within the system. The first half of our geometry is fuel, with the latter half being Steel, so we enter Fuel and Steel. The code will always read the same number of values here as the number of regions in the problem.</li> <li>Boundary Conditions - This tells the code what boundaries exist at the edges of our problem. Two boundary conditions have been implemented in out code, that of 'Zero' and 'Reflective'. The former simply ensures that the flux will tend to zero at the boundary, while the latter ensures that the derivative of the flux will tend to zero at the boundary.</li> </ul>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#reading-output-files","title":"Reading Output Files","text":"<p>The code generates two output files, OutputFile.txt and OutputFile.vtu. The former is a simple text file containing the position and flux of the solution to the problem. These are simply given in two columns such that they can be read easily by something like a GNUPlot or Python script. An example of such a flux profile can be seen below:</p> <pre><code>0.000000E+00  0.135813E+01\n0.166667E+00  0.137306E+01\n0.333333E+00  0.141907E+01\n0.500000E+00  0.150000E+01\n0.666667E+00  0.158093E+01\n0.833333E+00  0.162694E+01\n0.100000E+01  0.164187E+01\n</code></pre> <p>The results can easily then be visualised using Python, Excel, GNUPlot or any other drawing tool.</p> <p></p> <p>The OutputFile.vtu file stores additional data such as the region and cell numbers in an XML format. This can be directly read by software such as ParaView, allowing for more interactive visualisations than that of the previous flux profile. For visualisation purposes, the data has been smeared in a second dimension, which should give users an idea of how multidimensional cell data can be viewed. An example output from ParaView can be seen in the image below by running the following command in a terminal:</p> <pre><code>paraview OutputFile.vtu\n</code></pre> <p></p>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/diffusion/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 app\n\u2502   \u2514\u2500\u2500 main.F90\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 environment.yml\n\u251c\u2500\u2500 fpm.toml\n\u251c\u2500\u2500 input.in\n\u251c\u2500\u2500 install\n\u2502   \u251c\u2500\u2500 bin\n\u2502   \u2502   \u2514\u2500\u2500 diffusion\n\u2502   \u251c\u2500\u2500 include\n\u2502   \u2514\u2500\u2500 lib\n\u2502       \u251c\u2500\u2500 libdiffusion.a\n\u2502       \u2514\u2500\u2500 libdiffusion.so\n\u251c\u2500\u2500 install.sh\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 solutions\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 Constants.F90\n\u2502   \u251c\u2500\u2500 CRS.F90\n\u2502   \u251c\u2500\u2500 Materials.F90\n\u2502   \u251c\u2500\u2500 MatGen.F90\n\u2502   \u251c\u2500\u2500 Matrix_Base.F90\n\u2502   \u251c\u2500\u2500 Output.F90\n\u2502   \u251c\u2500\u2500 PETSc\n\u2502   \u2502   \u251c\u2500\u2500 PETSc_Init.F90\n\u2502   \u2502   \u251c\u2500\u2500 PETSc_Ksp.F90\n\u2502   \u2502   \u251c\u2500\u2500 PETSc_Mat.F90\n\u2502   \u2502   \u2514\u2500\u2500 PETSc_Vec.F90\n\u2502   \u251c\u2500\u2500 PETScSolver.F90\n\u2502   \u251c\u2500\u2500 Problem.F90\n\u2502   \u2514\u2500\u2500 Solver.F90\n\u2514\u2500\u2500 tools\n    \u251c\u2500\u2500 fluxplot\n    \u2514\u2500\u2500 plot.py\n</code></pre>","tags":["Nuclear Physics","Object-Oriented Programming","CMake","PETSc","Visualisation","Optimisation","Physics","Paraview","Finite Difference Method"]},{"location":"exemplars/environmentlit/","title":"Environmental Literature Analysis with BERTopic & RoBERTa","text":"Redirecting to exemplar docs...","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#recode-analysis-of-environmental-literature-with-bertopic-and-roberta","title":"ReCoDE - Analysis of environmental literature with BERTopic and RoBERTa","text":"","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#explosive-literature-in-environmental-and-sustainability-studies","title":"Explosive literature in Environmental and Sustainability Studies","text":"<p>The field of environmental and sustainability studies has witnessed an explosive growth in literature over the past few decades, driven by the increasing global awareness and urgency surrounding environmental issues, climate change, and the need for sustainable practices.</p> <p>This rapidly expanding body of literature is characterized by its interdisciplinary nature, encompassing a wide range of disciplines such as ecology, climate science, energy, economics, policy, sociology, and more. With a global focus and contributions from countries around the world, the literature base reflects diverse cultural, socio-economic, and geographical contexts, often in multiple languages. Novel research areas and emerging topics, such as circular economy, sustainable urban planning, environmental justice, biodiversity conservation, renewable energy technologies, and ecosystem services, continue to arise as environmental challenges evolve and our understanding deepens. The development of environmental policies, regulations, and international agreements, as well as increased public interest and awareness, have further fueled research and the demand for literature aimed at informing and engaging various stakeholders. Technological advancements in areas like remote sensing, environmental monitoring, and computational modelling have enabled new avenues of research and data-driven studies, contributing to the proliferation of literature. The rise of open access publishing and digital platforms has facilitated the dissemination and accessibility of this constantly evolving and interdisciplinary body of knowledge.</p> <p>So, in summary, the explosive growth of the literature across multiple disciplines, geographic regions, languages, and emerging topics poses significant challenges in terms of effectively organizing, synthesizing, and extracting insights from this vast and rapidly expanding body of knowledge. This is where Natural Language Processing (NLP) techniques like topic modelling with BERTopic and advanced language models like RoBERTa can play a crucial role. Their ability to process large volumes of text data, identify semantic topics and patterns, cluster related documents, and handle multiple languages can help researchers, policymakers, and stakeholders navigate this extensive literature more effectively.</p> <p>Furthermore, as a STEMM PhD student at Imperial stepping into a new field such as Sustainability, taking advantage of the NLP tools can significantly enhance the efficiency of literature exploration and review. This skill facilitates a seamless transition into interdisciplinary research, empowering you to navigate diverse datasets and extract valuable insights with greater ease and precision.</p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#the-potential-of-topic-modelling","title":"The Potential of Topic Modelling","text":"<p>Topic modelling is a technique in NLP and machine learning used to discover abstract \"topics\" that occur in a collection of documents. The key idea is that documents are made up of mixtures of topics, and that each topic is a probability distribution over words.</p> <p>More specifically, topic modelling algorithms like Latent Dirichlet Allocation (LDA) work by:</p> <ol> <li>Taking a set of text documents as input.</li> <li>Learning the topics contained in those documents in an unsupervised way. Each topic is represented as a distribution over the words that describe that topic.</li> <li>Assigning each document a mixture of topics with different weights/proportions.</li> </ol> <p>For example, if you ran topic modelling on a set of news articles, it may discover topics like \"politics\", \"sports\", \"technology\", etc. The \"politics\" topic would be made up of words like \"government\", \"election\", \"policy\" with high probabilities. Each document would then be characterized as a mixture of different proportions of these topics.</p> <p>The key benefits of topic modelling include:</p> <ol> <li>Automatically discovering topics without need for labeled data</li> <li>Understanding the themes/concepts contained in large document collections</li> <li>Organizing, searching, and navigating over a document corpus by topics</li> <li>Providing low-dimensional representations of documents based on their topics</li> </ol> <p>Topic modelling has found applications in areas like information retrieval, exploratory data analysis, document clustering and classification, recommendation systems, and more. Popular implementations include Latent Dirichlet Allocation (LDA), Biterm Topic Model (BTM), and techniques leveraging neural embeddings like BERTopic.</p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this tutorial, students will be able to acquire the following learning outcomes:</p> <ol> <li> <p>Proficiency in Text Data Preprocessing: Participants will gain hands-on experience in preprocessing environmental literature datasets, including cleaning, tokenisation, and normalisation techniques, essential for preparing data for NLP analysis.</p> </li> <li> <p>Understanding the principle of embedding-matrix-based NLP techniques: Through the application of BERTopic for topic modelling and RoBERTa for sentiment analysis, students will develop a deep understanding of advanced NLP methods and their practical implementation in dissecting environmental and sustainability texts and beyond.</p> </li> <li> <p>Critical Analysis Skills: Participants will learn to critically analyse and interpret the results of NLP analyses, including identifying dominant themes, sentiment shifts, and trends in environmental literature, fostering a nuanced understanding of environmental discourse.</p> </li> <li> <p>Interpretation and Application: Relying on a real-world example, this project demonstrates how to generate visualisations and reports to present the results of the topic modelling and sentiment analysis, facilitating interpretation and discussion.</p> </li> </ol>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#requirements","title":"Requirements","text":"<p>It would help a lot if you went through the following Graduate School courses before going through this exemplar: * Introduction to Python * Data Exploration and Visualisation * Data Processing with Python Pandas * Plotting in Python with Matplotlib * Binary Classification of Patent Text Using Natural Language Processing (another ReCoDE project)</p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#academic","title":"Academic","text":"<ul> <li>Access to Google Colaboratory</li> <li>Basic Math (matrices, averages)</li> <li>Programming skills (python, pandas, numpy, tensorflow)</li> <li>Machine learning theory (at level of intro to machine learning course)</li> </ul>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#system","title":"System","text":"<p>Windows, MacOS, Ubuntu Python 3.11 or higher Ideally with GPU for fast running of the code</p> <p>NB: If you have access to High Performance Computing (HPC), we have prepared a specially adapted file for Imperial HPC environments, located under the \"notebook\" directory. This file is optimized to leverage the computational power and resources available through HPC, enabling more efficient processing and faster execution of your tasks.</p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#getting-started","title":"Getting Started","text":"","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#colab","title":"Colab","text":"<p>Please visit this Colab page to access the detailed content of this tutorial: https://colab.research.google.com/drive/1vJzmFTFurlK-NGDw_fhJgxSmcKSZooLn?usp=sharing</p> <p> </p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#a-step-by-step-case-study-using-bertopic-to-analyze-one-web-of-science-dataset","title":"A Step-by-Step Case Study using BERTopic to Analyze One web of Science Dataset","text":"<p>In this step-by-step case study, we will focus on the application of BERTopic, to analyze a sample dataset sourced from Web of Science. Through this tutorial, we aim to guide you through the process:</p> <ul> <li>Installation and setup of BERTopic</li> <li>Collecting the raw data and preprocessing the dataset</li> <li>Implementing BERTopic for topic modeling</li> <li>Visualizing the inferred topics and interpreting the results</li> <li>Fine-tuning topic representations</li> <li>Additional readings about the wider application of BERTopic</li> </ul> <p>By following along, you will gain practical insights into leveraging BERTopic for insightful analysis of scholarly literature from Web of Science.</p> <p>Some sample visualisation results can be:  </p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#a-step-by-step-case-study-using-roberta","title":"A Step-by-Step Case Study using RoBERTa","text":"<p>Similar to what we have done above, we need to follow the following steps when applying a RoBERTa model.</p> <ul> <li>RoBERTa Initialization: Initializes RoBERTa tokenizer and model</li> <li>Data Preparation: Loads and preprocesses the dataset</li> <li>Batch Tokenization: Tokenizes abstracts in batches</li> <li>Embedding Generation: Generates embeddings using RoBERTa, and save it</li> <li>Topic Modeling: Applies BERTopic with RoBERTa embeddings</li> <li>Improve and fine-tune</li> <li>Visualization</li> </ul> <p>This section focuses on integrating RoBERTa into the topic modeling pipeline, enhancing its analytical capabilities.</p> <p>Some sample visualisation results can be:  </p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#frequently-asked-questions-and-best-practices-all-details-can-be-found-on-the-colab-page","title":"Frequently Asked Questions and Best Practices (All details can be found on the Colab page)","text":"<p>https://colab.research.google.com/drive/1vJzmFTFurlK-NGDw_fhJgxSmcKSZooLn#scrollTo=SbhVeLI72Idj&amp;line=1&amp;uniqifier=1 </p> <ol> <li>Specify a preferred hardware accelerator on Colab</li> <li>Besides Web of Science, where else can I find datasets, and how can I import literature datasets in bulk across platforms?</li> <li>Can I publish the textual dataset I pre-processed, and where\uff1f</li> <li>Why do we need to explore literature and compared to manual exploration, what are the advantages of applying models like BERT?</li> <li>Factors helping you decide whether to apply RoBERTa (if you have used a general BERToic model):</li> <li>Why is preprocessing necessary when using RoBERTa but not always required when using BERTopic\uff1f</li> <li>Why is it necessary to pretrain a RoBERTa model?</li> <li>What happens next after pretraining?</li> </ol>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#further-reading","title":"Further Reading","text":"<ul> <li>The BERTopic developer's github page: https://github.com/MaartenGr/BERTopic</li> <li>The BERTopic developer's personal blog page: https://www.maartengrootendorst.com/</li> <li>Tutorial page - Leveraging BERT and c-TF-IDF to create easily interpretable topics: maartengr.github.io/BERTopic/</li> <li>Natural Language Processing: A Textbook with Python Implementation (by Raymond S. T. Lee): https://www.amazon.co.uk/Natural-Language-Processing-Textbook-Implementation-ebook/dp/B0CBR29GV2</li> <li>Speech and Language Processing (3rd ed. draft) (by Dan Jurafsky and James H. Martin): https://web.stanford.edu/~jurafsky/slp3/</li> <li>Multi-lingual and multi-modal topic modelling with pre-trained embeddings: https://aclanthology.org/2022.coling-1.355.pdf</li> </ul>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 .github                   &lt;- github Actions workflows\n\u2502\n\u251c\u2500\u2500 docs                      &lt;- documents\n\u2502   \u251c\u2500\u2500 .icons/logos             &lt;- icon\n\u2502   \u251c\u2500\u2500 datasets                 &lt;- raw datasets\n\u2502       \u2502\u2500\u2500 Web_of_Science_Query May 07 2024_1-5000.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_1-1000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_1001-2000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_2001-3000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_3001-4000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_4001-5000 results.csv\n\u2502   \u251c\u2500\u2500 plots                 &lt;- plots\n\u2502       \u2502\u2500\u2500 hier_cluster_bert_1.png\n\u2502       \u2502\u2500\u2500 hier_cluster_roberta_1.png\n\u2502       \u2502\u2500\u2500 hier_cluster_roberta_2.png\n\u2502       \u2502\u2500\u2500 inter-topic_bert1.png\n\u2502       \u2502\u2500\u2500 inter-topic_roberta1.png\n\u2502       \u2502\u2500\u2500 inter-topic_roberta2.png\n\u2502       \u2502\u2500\u2500 sim_max_bert_1.png\n\u2502       \u2502\u2500\u2500 sim_max_roberta_2.png\n\u2502       \u2502\u2500\u2500 top_topics_bert_1.png\n\u2502       \u2502\u2500\u2500 top_topics_roberta_2.png\n\u2502   \u251c\u2500\u2500 BERT_Walkthrough.md\n\u2502   \u251c\u2500\u2500 CARBON.md\n\u2502   \u251c\u2500\u2500 FAQ.md\n\u2502   \u251c\u2500\u2500 Readings.md\n\u2502   \u251c\u2500\u2500 RoBERTa_Walkthrough.md\n\u2502   \u251c\u2500\u2500 data_sources.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 key_concepts.md\n\u251c\u2500\u2500 notebooks                 &lt;- project data\n\u2502       \u251c\u2500\u2500 ReCoDE_Analysis_of_environmental_literature_with_BERTopic_and_RoBERTa_colab.ipynb          &lt;- Saved .ipynb file from colab\n\u2502       \u251c\u2500\u2500 ReCoDE-BERTopic&amp;RoBERTa_Run on HPC at Imperial.ipynb                                 &lt;- Saved .ipynb file suitable for running on High-Performance Computer\n\u251c\u2500\u2500 mkdocs.yml                \n\u251c\u2500\u2500 requirements.txt          &lt;- file for installing python dependencies\n\u251c\u2500\u2500 LICENSE.md\n\u2514\u2500\u2500 README.md\n</code></pre>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/environmentlit/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Topic Modelling","BERTopic","RoBERTa","Natural Language Processing"]},{"location":"exemplars/eulermaruyama/","title":"Solving SDEs with Euler-Maruyama","text":"Redirecting to exemplar docs...","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#recode-project-euler-maruyama-method","title":"ReCoDE project - Euler-Maruyama method","text":"","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#description","title":"Description","text":"<p>This code is part of the Research Computing and Data Science Examples (ReCoDE) projects.  The project consists of a Python class containing the Euler-Maruyama (EM) method for the numerical solution of a Stochastic Differential Equation (SDE). SDEs describe the dynamics that govern the time-evolution of  systems subjected to deterministic and random influences. They arise in fields such as biology, physics or  finance to model variables exhibiting uncertain and fluctuating behaviour. Being able to numerical solve an SDE  is essential for these fields, especially if there is no closed-form solution. This project provides an  object-oriented implementation of the EM method. Throughout the project, it is emphasised the benefits that class encapsulation provides in terms of code modularity and re-usability.</p>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#learning-outcomes","title":"Learning Outcomes","text":"<p>This project is designed for Master's and Ph.D. students with basic Python knowledge and need to solve SDEs for their research projects. After going through this project, students will:</p> <ol> <li>Understand how to solve an SDE using the EM method.</li> <li>Learn to encapsulate the EM method code into a Python class.</li> <li>Explore how to parallelise the code to improve solution speed.</li> </ol>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#requirements","title":"Requirements","text":"","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#system","title":"System","text":"Program Version Git &gt;= 2.41 Python &gt;= 3.9","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#dependencies","title":"Dependencies","text":"Packages Version poetry &gt;= 1.4.* numpy &gt;= 1.24.* matplotlib &gt;= 3.7.* jupyter &gt;= 1.0.* joblib &gt;= 1.2.*","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 .github/workflows\n\u2502   \u2514\u2500\u2500 tests_workflow.yaml\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 1-Introduction.ipynb\n\u2502   \u251c\u2500\u2500 2-Probability-Distributions.ipynb\n\u2502   \u251c\u2500\u2500 3-Euler-Maruyama-Method.ipynb\n\u2502   \u251c\u2500\u2500 4-Euler-Maruyama-Class.ipynb\n\u2502   \u2514\u2500\u2500 5-Parallel-Euler-Maruyama-Class.ipynb\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 euler_maruyama\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 coefficients.py\n\u2502       \u251c\u2500\u2500 euler_maruyama.py\n\u2502       \u2514\u2500\u2500 parallel_euler_maruyama.py\n\u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 test_coefficient.py\n\u2502   \u251c\u2500\u2500 test_euler_maruyama.py\n\u2502   \u2514\u2500\u2500 test_parallel_euler_maruyama.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 poetry.toml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#getting-started","title":"Getting Started","text":"<p>You can read the Jupyter notebooks non-interactively on Github. Click here to view the collection of Jupyter notebooks located in the <code>docs</code> folder. However, for an improved experience, we suggest cloning the Github repository and running the Jupyter notebooks on your local machine. To assist you setting up the project locally, we provide a list of steps:</p>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#1-clone-the-repository","title":"1. Clone the repository","text":"<p>After installing <code>git</code> in your local machine, you can run the following command in a terminal:</p> <pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDe_Euler_Maruyama.git euler-maruyama\ncd euler-maruyama\n</code></pre>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#2-install-poetry","title":"2. Install poetry","text":"<p>Once you have downloaded a <code>Python</code> version, you need to install <code>poetry</code>.  <code>Poetry</code> is a dependency management and packaging tool for <code>Python</code> projects that simplifies the process of managing dependencies and distributing packages. It allows you to define project dependencies in a <code>pyproject.toml</code> file and provides commands to install, update, and remove dependencies.  The main advantages of <code>poetry</code> include dependency resolution to ensure consistent environments, the management of virtual environments for isolation and simplified package publishing.  It streamlines the development workflow and facilitates collaboration by providing a unified and straightforward approach to managing dependencies in <code>Python</code> projects. You can find more information in its documentation. Our main focus here is to use <code>poetry</code> to install the project and their dependencies locally.</p> <pre><code>pip install poetry\n</code></pre> <p>You can check that <code>poetry</code> has been successfully installed by running:</p> <pre><code>poetry --version\nPoetry (version 1.4.0)\n</code></pre>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#3-install-the-project","title":"3. Install the project","text":"<p>Now, we need to install the project and its requirements. You can run the following command in the folder  where you downloaded the Github repository:</p> <pre><code>poetry install\n</code></pre> <p>This command creates a virtual environment in the same folder you are working, as specified by the <code>poetry.toml</code> configuration file of the project. Then, the packages requirements are installed and finally, the project is installed locally with the name <code>euler-maruyama</code> version (0.1.0).</p>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#4-activate-the-local-environment","title":"4. Activate the local environment","text":"<p>Run the following command to activate the local environment you created in the previous step:</p> <pre><code>poetry shell\n</code></pre>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#5-launch-the-jupyter-notebooks","title":"5. Launch the Jupyter notebooks","text":"<p>You can run this command to launch the Jupyter notebook:</p> <pre><code>jupyter notebook\n</code></pre> <p>Now, you can explore and experiment with the different notebook examples we have prepared to help you understand this project.</p>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/eulermaruyama/#6-close-the-environment","title":"6. Close the environment","text":"<p>If you have closed the Jupyter notebooks and want to exit from the local environment, just run:</p> <pre><code>exit\n</code></pre>","tags":["Object-Oriented Programming","NumPy","Stochastic Differential Equations","Uncertainty Quantification","Probability Distributions","Matplotlib"]},{"location":"exemplars/finiteelement/","title":"Finite Element Method","text":"Redirecting to exemplar docs...","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#numerical-simulation-using-finite-element-method","title":"Numerical Simulation using Finite Element Method","text":"","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#description","title":"Description","text":"<p>Finite Element Method (FEM) is a powerful computational method used across a wide variety of disciplines, including engineering, mathematics and earth science. It is used to solve complex differential equations and simulate real-world scenarios. This project focuses on application of FEM in solid mechanics, but the teachings can be extended to other areas of study. It provides a comprehensive, step-by-step tutorial designed to introduce users to the fundamentals of FEM and its practical implementation in Python. The tutorial takes users through all the key stages of numerical simulations, starting from domain meshing and problem discretisation, moving to solving the differential equations and finally analysing and understanding the results. To make the learning process intuitive, the project works through two examples of increasing difficulty. The first example is straightforward and demonstrates the methodology and code implementation. As users progress, they are gradually introduced to more advanced aspects of FEM, ensuring a solid foundation and deeper understanding of this method.</p>","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Develop understanding of the fundaments of FEM</li> <li>Learn how to create custom FEM code and adapt it to solve partial differential equations</li> <li>Gain familiarity with existing libraries and tools available for FEM implementation</li> <li>Develop skills to interpret solutions and effectively analyse the simulation results</li> </ul> <p>In this project, the exercises are integrated with the theoretical components.</p> Task Time Reading 3 hours Practising 3 hours","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#requirements","title":"Requirements","text":"","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#academic","title":"Academic","text":"<p>To successfully complete this examplar the user should have basic understanding of key mathematical concepts, including linear algebra (such as matrices, vectors, and determinants) and differential equations (both ordinary and partial differential equations, linear system of equations)</p> <p>Familiarity with Python programming, using existing libraries, writing classes, functions and graph plotting.</p>","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#system","title":"System","text":"<p>System requirements: - Python 3.11 or newer - Paraview 4.0 or newer - Jupyter Notebooks</p> <p>Python libraries used: - pygmsh - numpy - matplotlib</p>","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#getting-started","title":"Getting Started","text":"","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#finite-element-method-overview","title":"Finite Element Method Overview","text":"<p>A significant part of our understanding of the real world comes from differential equations, which describe the behaviour of various quantities. These equations are fundamental across many scientific fields, including physics, geoscience, and medicine, as they explain how systems behave under specific conditions, how they respond to external forces and change with time.</p> <p>Solving differential equations allows us to predict and analyse real world phenomena. In many cases, computer simulations are used to solve these equations for specific scenarios. Engineers use computer simulations to optimize product designs, ensuring they can withstand real world conditions, while geoscientists use them to study complex natural processes or our interactions with them, such as energy storage. These simulations enable us to test hypotheses and evaluate outcomes without the need for expensive real life experiments. However, their accuracy and reliability depend on our ability to effectively solve the underlying systems of differential equations.</p> <p>In certain simplified cases, differential equations can be solved analytically, providing exact solutions. However, these cases are often over simplified and may not be flexible enough for practical applications. When analytical solutions are not feasible, we use numerical methods to approximate solutions. A wide range of numerical techniques exists, each designed to solve specific problems and improve the accuracy and efficiency of the solutions.</p> <p>One of the most widely used numerical methods is the Finite Element Method (FEM). It is particularly popular in mechanics, including structural and rock mechanics, fluid dynamics, and heat transfer. Since its introduction in the 1950s, FEM has become an essential tool in numerical modeling, with extensive research dedicated to refining and advancing its capabilities.  </p> <p>The core principle of FEM is discretising the problem domain into smaller interconnected elements. This process, known as discretisation, allows the problem to be described at the element level. Each element has an associated number of nodes, which are shared by adjacent elements. These nodes form relationships between elements that can be aggregated to a global system of equations. Solving this system yields an approximate solution at the nodes across the domain. Interpolation then allows us to calculate the solution at any point.</p> <p>Therefore the key stages of Finite Element Methods are:</p> <ul> <li>Domain discretisation  -  dividing the domain into finite elements</li> <li>Element level problem formulation - defining the equations on each element using basis functions</li> <li>Assembly to global system - combining the element equations into a global system</li> <li>Application of boundary conditions - applying constrains and conditions to the problem to solve for unique solutions.</li> <li>Solving linear system of equations - using a numerical solver to find the solution</li> <li>Post processing of solution - interpreting, visualising and analysing the solution</li> </ul>","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#project-overview","title":"Project Overview","text":"<p>The project is split into two main examples. Inside each examples there are smaller exercises that allow the user to apply and practice the theoretical components presented. For some exercises the user is expected to complete the code. The missing parts are indicated with <code>...</code> and the solutions are provided in the hidden cells marked \"Solution\" in red. By double-clicking on the hidden cells, the user can reference solutiona and copy and past into the code cells.</p> <p>This exercise will guide you through the key stages of constructing a Finite Element code. It will focus on a simple problem of heat transfer to build the basics of the method. At the end of this exercise you should have a good understanding of how FEM works.</p> <p>The next exercise will build on these concepts and look at solving a slightly more complex problem: an engineering problem where we solve for displacement vector. In this exercise we focus on applying different type of boundary conditions and analysing solution convergence to analytical solution.</p>","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#project-structure","title":"Project Structure","text":"<pre><code>\u2502   .gitignore\n\u2502   LICENSE.md\n\u2502   mkdocs.yml\n\u2502   README.md\n\u2502   requirements.txt\n\u2502\n\u251c\u2500\u2500\u2500.devcontainer\n\u2502       devcontainer.json\n\u2502       Dockerfile\n\u2502\n\u251c\u2500\u2500\u2500.github\n\u2502   \u2514\u2500\u2500\u2500workflows\n\u2502           docs.yml\n\u2502           link_checker.yml\n\u2502\n\u251c\u2500\u2500\u2500.ipynb_checkpoints\n\u251c\u2500\u2500\u2500docs\n\u2502   \u2502   index.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500\u2500.icons\n\u2502   \u2502   \u2514\u2500\u2500\u2500logos\n\u2502   \u2502           iclogo.svg\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500assets\n\u2502           iclogo.png\n\u2502\n\u251c\u2500\u2500\u2500FEM_Module\n\u2502   \u2502   FEM_Module_class.py\n\u2502   \u2502   Run_Cantilever_Example.py\n\u2502   \u2502   Support_functions.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500__pycache__\n\u2502           FEM_Module_class.cpython-311.pyc\n\u2502           Run_Cantilever_Example.cpython-311.pyc\n\u2502           Support_functions.cpython-311.pyc\n\u2502\n\u2514\u2500\u2500\u2500notebooks\n    \u2502   .placeholder\n    \u2502   Part_1.ipynb\n    \u2502   Part_2.ipynb\n    \u2502\n    \u251c\u2500\u2500\u2500.ipynb_checkpoints\n    \u2502       Part_1-checkpoint.ipynb\n    \u2502       Part_2-checkpoint.ipynb\n    \u2502       Part_2_withlinear-checkpoint.ipynb\n    \u2502\n    \u2514\u2500\u2500\u2500img\n            Cantilever_diagram.png\n            cantilever_result.png\n            element_in_mesh.png\n            element_types.png\n            hat_functions_as_basis.png\n            isoparametric_triangle.png\n            line_element_diag.png\n            mapping_to_isoparametric.png\n            rectangularmesh.png\n            rectangular_mesh_simple.png\n            temp_result.png\n            triangular_mesh.png\n</code></pre>","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/finiteelement/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Finite Element Analysis","Partial Differential Equations","Mesh","Boundary Conditions","Convergence Analysis"]},{"location":"exemplars/geneanalysis/","title":"Gene Network Analysis","text":"Redirecting to exemplar docs...","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#single-cell-gene-network-analysis","title":"Single-Cell Gene Network Analysis","text":"","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#description","title":"Description","text":"<p>The aim of this exemplar is to understand and demonstrate how to carry out single-cell gene network analysis on real data. Specifically, we will model networks in a small, publicly available Human Cancer dataset as well as explore them further through downstream analysis. This will consist of characterising the networks through gene set enrichment analysis and understanding the principles behind this. The modelling itself will encompass learning how to use both correlation-based approaches as well as deep learning techniques, such as large language models to construct networks. Together, this will provide an undertanding as well as practical experience with an end-to-end pipeline using single-cell data for gene network analysis.</p>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#overall-learning-outcomes","title":"Overall Learning Outcomes","text":"<p>By the end of the series of chapters and notebooks you should be able to:</p> <ol> <li>Understand the Foundations of Single-Cell RNA-Sequencing (scRNA-seq):</li> <li>Comprehend the steps involved in scRNA-seq data processing, including generating count     matrices and performing quality control.</li> <li>Explain how scRNA-seq differs from bulk RNA-seq in capturing cellular heterogeneity.</li> <li>Explore publicly available snRNA-seq data and process it for downstream analysis using Python.</li> <li>Apply Techniques in Gene Network Analysis:</li> <li>Understand the concepts and methods involved in constructing gene networks from scRNA-seq data.</li> <li>Identify relationships between genes using co-expression based approaches.</li> <li>Visualise gene networks using tools such as NetworkX.</li> <li>Carry Out Gene Set Enrichment Analysis:</li> <li>Explain the significance of gene set enrichment and its role in interpreting gene networks.</li> <li>Perform pathway-based enrichment analyses using curated gene sets from resources like KEGG and Gene Ontology (GO).</li> <li>Apply statistical tests such as the Fisher Exact Test to assess the overrepresentation of gene sets.</li> <li>Implement multiple hypothesis correction methods like the false discovery rate (FDR) to ensure robust enrichment results.</li> <li>Utilise Large Language Models for Gene Network Predictions:</li> <li>Explain the basic principles of NLP, including tokenization, stemming, and dependency parsing.</li> <li>Recognize the capabilities of LLMs in analyzing complex biological datasets, even when data is limited.</li> <li>Apply pretrained models like scGPT to predict gene interactions and network dynamics in scRNA-seq data.</li> <li>Understand the role of transformers and attention mechanisms in enhancing biological predictions through context-aware learning.</li> </ol> Task Time Reading 3 hours Practising 3 hours","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#requirements","title":"Requirements","text":"<ul> <li>Familiarity with Python</li> <li>Basic knowledge on NLP and LLMs</li> <li>Basic knowledge on single-cell RNA-sequencing</li> </ul>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#getting-started","title":"Getting Started","text":"<p>Take a look at the table of contents below and follow the chapters from 1 to 4. These are also aligned with the notebooks and should give you background information as well as direction and motivation behind the notebooks. If you are ever unsure or don't understand how to do something on the workbooks, there are also fully worked through solutions that you can use.</p>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#cloning-the-repository","title":"Cloning the repository","text":"<p>If you don't have it already, install git to your machine, see here for details on all OS's.</p> <p>Once installed, run the following to clone the repository and change the working directory to the clones repository:</p> <pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-Gene-Network-Analysis.git\ncd ReCoDE-Gene-Network-Analysis\n</code></pre>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#setting-up-your-environment","title":"Setting up your environment","text":"<p>Before installing the project dependencies, it's recommended to set up a virtual environment. This ensures that the dependencies for this project do not interfere with those of other Python projects you may be working on. Follow these steps to create and activate a virtual environment:</p> <ol> <li>Create a Virtual Environment</li> </ol> <p>If you haven't already, navigate to the root directory of the project and run the following command to create a virtual environment. This command creates a new directory <code>.venv</code> which will contain all the necessary executables to use the packages that a Python project would need.</p> <pre><code>python3 -m venv .venv\n</code></pre> <ol> <li>Activate the Virtual Environment</li> </ol> <p>After creating the virtual environment, you need to activate it. Activation of the virtual environment will change your shell\u2019s prompt to show what virtual environment you\u2019re using, and modify the environment so that running python will get you that particular version and installation of Python.</p> <ul> <li>On Windows</li> </ul> <pre><code>.\\venv\\Scripts\\activate\n</code></pre> <ul> <li>On Unix or MacOS</li> </ul> <pre><code>source venv_recode_GeneNetworkAnalysis/bin/activate\n</code></pre> <p>This command needs to be run every time you start a new terminal session and want to work on this project.</p> <ol> <li>Install Dependencies</li> </ol> <p>With the virtual environment activated, install the project dependencies by running:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Remember to deactivate your virtual environment when you're done working on the project by running deactivate.</p>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#system","title":"System","text":"","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#development","title":"Development","text":"<p>This project uses <code>pre-commit</code> and <code>pip-tools</code> for managing pre-commit hooks and dependencies respectively. Follow these steps to set up your development environment:</p>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#setting-up-pre-commit","title":"Setting up pre-commit","text":"<ol> <li>Install pre-commit on your system. If you haven't installed it yet, you can do so by running:</li> </ol> <pre><code>pip install pre-commit\n</code></pre> <ol> <li>Once installed, you can set up the pre-commit hooks by running the following command in the root directory of this project:</li> </ol> <pre><code>pre-commit install\n</code></pre> <p>This will install all the pre-commit hooks defined in .pre-commit-config.yaml into your local repository. These hooks will automatically run on every commit to ensure code quality and consistency.</p>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#managing-dependencies-with-pip-tools","title":"Managing Dependencies with pip-tools","text":"<p>Dependencies are managed using the pip-tools tool chain.</p> <p>Unpinned dependencies are specified in <code>pyproject.toml</code>. Pinned versions are then produced with:</p> <pre><code>pip-compile pyproject.toml\n</code></pre> <p>To add/remove packages edit <code>pyproject.toml</code> and run the above command. To upgrade all existing dependencies run:</p> <pre><code>pip-compile --upgrade pyproject.toml\n</code></pre> <p>Dependencies for developers are listed separately as optional, with the pinned versions being saved to <code>dev-requirements.txt</code> instead.</p> <p><code>pip-tools</code> can also manage these dependencies by adding extra arguments, e.g.:</p> <pre><code>pip-compile -o dev-requirements.txt --extra=dev pyproject.toml\n</code></pre> <p>When dependencies are upgraded, both requirements.txt and dev-requirements.txt should be regenerated so that they are in sync.</p>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 .github/workflows\n\u251c\u2500\u2500 data\n|   \u251c\u2500\u2500 data\n|       \u251c\u2500\u2500 Bcell_datExpr_pseudobulk.csv\n\u2502       \u2514\u2500\u2500 Bcell_filtered_compressed.h5ad.gz\n|   \u251c\u2500\u2500 metadata\n|       \u251c\u2500\u2500 Bcell_metadata.csv\n\u2502       \u2514\u2500\u2500 Bcell_metadata_pseudobulk.csv\n\u2502   \u2514\u2500\u2500 other\n|       \u251c\u2500\u2500 h.all.v2023.2.Hs.symbols.gmt\n\u2502       \u2514\u2500\u2500 separated_communities.pkl\n\u251c\u2500\u2500 docs\n|   \u251c\u2500\u2500 .icons/logos\n|   \u251c\u2500\u2500 assets\n|   \u251c\u2500\u2500 Chapter 1: Background on Single-Cell RNA-Sequencing\n|   \u251c\u2500\u2500 Chapter 2: Gene Network Analysis\n|   \u251c\u2500\u2500 Chapter 3: Gene Network Downstream Analysis\n\u2502   \u2514\u2500\u2500 Chapter 4: Understanding NLP and LLMs\n\u251c\u2500\u2500 notebooks\n|   \u251c\u2500\u2500 solutions\n|       \u251c\u2500\u2500 1.Correlation_Network_Analysis_and_Visualisation_Solution.ipynb\n|       \u251c\u2500\u2500 2.Network_Analysis_Solution.ipynb\n|       \u251c\u2500\u2500 3.Fisher_Exact_Test_Solution.ipynb\n\u2502       \u2514\u2500\u2500 4.SCGPT_Solution.ipynb\n|   \u251c\u2500\u2500 workbooks\n|       \u251c\u2500\u2500 1.Correlation_Network_Analysis_and_Visualisation_Workbook.ipynb\n|       \u251c\u2500\u2500 2.Network_Analysis_Workbook.ipynb\n|       \u251c\u2500\u2500 3.Fisher_Exact_Test_Workbook.ipynb\n\u2502       \u2514\u2500\u2500 4.SCGPT_Workbook.ipynb\n|   \u251c\u2500\u2500 .placeholder\n|   \u251c\u2500\u2500 process_data.ipynb\n\u251c\u2500\u2500 test\n|   \u251c\u2500\u2500 test_placeholder.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 LICENSE.md # Outlines what legal rights you have to use this software.\n\u251c\u2500\u2500 README.md  # You are here!\n\u251c\u2500\u2500 dev-requirements.txt # Development requirements\n\u251c\u2500\u2500 mkdocs.yml # Tells readthedocs.com how to build the documentation.\n\u251c\u2500\u2500 pyproject.toml # Machine readable information about the package.\n\u251c\u2500\u2500 requirements.txt # Requirements for the notebooks.\n\u2514\u2500\u2500 test\n</code></pre>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/geneanalysis/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Single-Cell RNA Sequencing","Methods for Single-Cell Gene Network Analysis","Gene Set Enrichment Analysis","Visualisation","Natural language processing","Data analysis"]},{"location":"exemplars/hamiltonianjax/","title":"Hamiltonian Systems in JAX","text":"Redirecting to exemplar docs...","tags":["JAX","Object-Oriented Programming","Ordinary Differential Equations","Matplotlib","Dynamic Systems"]},{"location":"exemplars/hamiltonianjax/#hamiltonian-systems-in-jax","title":"Hamiltonian systems in Jax","text":"<p>In this ReCoDE exemplar, we introduce the basics of Hamiltonian dynamics and demonstrate how Python and JAX can be used to simulate and visualise systems. We go over the mathematical prerequisites and create a framework for solving Hamiltonian equations using the general dynamical systems solver in JAX. Finally, we demonstrate how we can implement simple harmonic oscillator and N-body systems, including visualisation code to plot various system simulated over time.</p>","tags":["JAX","Object-Oriented Programming","Ordinary Differential Equations","Matplotlib","Dynamic Systems"]},{"location":"exemplars/hamiltonianjax/#learning-outcomes","title":"Learning Outcomes","text":"<p>By studying this exemplar you will be better able to:</p> <ul> <li>Use dynamical systems to model physical scenarios</li> <li>Use JAX to solve ODEs and perform linear algebra operations</li> <li>Visualise trajectories in 1D and 2D phase space</li> <li>Organise code in an an object-oriented way</li> </ul>","tags":["JAX","Object-Oriented Programming","Ordinary Differential Equations","Matplotlib","Dynamic Systems"]},{"location":"exemplars/hamiltonianjax/#estimated-duration","title":"Estimated Duration","text":"Task Time Reading 3 hours Practising 2 hours","tags":["JAX","Object-Oriented Programming","Ordinary Differential Equations","Matplotlib","Dynamic Systems"]},{"location":"exemplars/hamiltonianjax/#requirements","title":"Requirements","text":"<p>Undergraduate level calculus, linear algebra and a basic understanding of physics are required. Specific knowledge of Hamiltonian dynamics is not required, as we will introduce the necessary concepts in the course of this exemplar.</p>","tags":["JAX","Object-Oriented Programming","Ordinary Differential Equations","Matplotlib","Dynamic Systems"]},{"location":"exemplars/hamiltonianjax/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["JAX","Object-Oriented Programming","Ordinary Differential Equations","Matplotlib","Dynamic Systems"]},{"location":"exemplars/handeyecalib/","title":"Hand-eye Calibration of Medical Robots","text":"Redirecting to exemplar docs...","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#hand-eye-calibration-for-medical-robots","title":"Hand-eye Calibration for Medical Robots","text":"","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#description","title":"Description","text":"<p>Hand-eye calibration is a well-studied topic in the field of robotics. Applications that involve the use cameras typically require conducting hand-eye calibration beforehand. This project presents a pipeline for conducting hand-eye calibration using a bespoke marker for a medical robot (the first generation da Vinci research kit). The experimental setup is shown in Fig 1.</p> Fig 1. Experimental setup <p>The method presented in this project aims to conduct registration between two sets of point cloud using singular value decomposition (SVD). This project aims to instruct students to understand basic knowledge of hand-eye calibration and grasp essential skills in using computer vision libraries such as OpenCV and Point Cloud library in C++. Hand-eye calibration results can be visualised through 2D back projections. Fig 2 and Fig 3 display the back projection of a surgical tool shaft after accurate and inaccurate hand-eye calibrations, respectively. </p> Fig 2. Overlay with accurate calibration Fig 3. Overlay with inaccurate calibration","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Understand what hand-eye calibration is and prevalent school of thoughts in solving this problem.</li> <li>Develop basic skills in writing an object-oriented project in C++.</li> <li>Develop basic skills in using computer vision libraries, such as OpenCV and Point Cloud Library. </li> </ul> Task Time Reading 3 hours Practising 2 hours","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#requirements","title":"Requirements","text":"","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#academic","title":"Academic","text":"<ul> <li>Basic knowledge on robotics (eg. Denavit\u2013Hartenberg (DH) parameters, forward kinematics)</li> <li>Hand-eye calibration and fundamental knowledge on computer vision (eg. camera intrinsic matrix, extrinsic matrix, projection geometry)</li> <li>Linear algebra (eg. Singular Value Decomposition (SVD)). Detailed information can be referred to <code>docs/Background.md</code></li> </ul>","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#system","title":"System","text":"<ul> <li>A C++ toolchain along with the necessary development libraries:<ul> <li>Eigen library (Eigen 3)</li> <li>OpenCV library (OpenCV 4.6)</li> <li>Point cloud library (PCL 1.11)</li> </ul> </li> </ul>","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#getting-started","title":"Getting Started","text":"<p>Background knowledge on robotics and computer vision can be referred to <code>docs/Background.md</code>. The workflow of this project can be referred to <code>docs/Code_Overview.md</code>. <code>docs/EigenLibraryIntro.md</code>, <code>docs/OpenCVLibraryIntro.md</code> and <code>docs/PointCloudLibraryIntro.md</code> contain information on the basic usage of these C++ libraries, and explanations on related fuctions written in this project using these libraries. <code>docs/input.md</code> provides information of input files used in this project. </p>","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 IR\n\u2502   \u251c\u2500\u2500 RGB\n\u2502   \u251c\u2500\u2500 pcd\n\u2502   \u251c\u2500\u2500 Acusense_RGB_K.txt\n\u2502   \u251c\u2500\u2500 ExtrinsicMat.txt\n\u2502   \u251c\u2500\u2500 q_history.txt\n\u2502   \u251c\u2500\u2500 pos_act.txt\n\u2502   \u2514\u2500\u2500 pos_des.txt\n\u251c\u2500\u2500 output\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 main.cc\n\u2502   \u251c\u2500\u2500 FeatureDetection.cc\n\u2502   \u2514\u2500\u2500 utils.cc\n\u251c\u2500\u2500 include\n\u2502   \u251c\u2500\u2500 FeatureDetection.hpp\n\u2502   \u2514\u2500\u2500 utils.hpp\n\u251c\u2500\u2500 CMakeLists.txt \n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 Background.md\n\u2502   \u251c\u2500\u2500 Code_Overview.md\n\u2502   \u251c\u2500\u2500 EigenLibraryIntro.md\n\u2502   \u251c\u2500\u2500 FeatureDetection.md\n\u2502   \u251c\u2500\u2500 OpenCVLibrary.md\n\u2502   \u251c\u2500\u2500 PointClourLibraryIntro.md\n\u2502   \u2514\u2500\u2500 input.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE.md\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 doxide.yaml\n\u2514\u2500\u2500 requirements.txt \n</code></pre>","tags":["Robotics","Computer Vision"]},{"location":"exemplars/handeyecalib/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Robotics","Computer Vision"]},{"location":"exemplars/hiddenmarkov/","title":"Hidden Markov Models for behavioural states","text":"Redirecting to exemplar docs...","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#hidden-markov-models-for-the-discovery-of-behavioural-states","title":"Hidden Markov Models for the discovery of behavioural states","text":"","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#description","title":"Description","text":"<p>This is an exemplar project to help you understand the concepts behind the Hidden Markov Model (HMM), how to implement one with the python package hmmlearn, and finally how to explore the decoded data.</p> <p>HMMs are widely used in multiple fields, including biology, natural language processing, and finance as a predictor of future states in a sequence. However, here we will be utilising the hidden model states to create a hypothesied internal behavioural architecture.</p> <p>The tutorial will also run briefly through how to clean and augment a real world dataset using numpy and pandas, so that it's ready for training with hmmlearn.</p> <p>The information in tutorial was primarily designed around the user completing reading along and completing a jupyter notebook in python. But can followed loosely from just these pages. If reading along ignore any sections asking to complete any code (or complete it in your mind).</p> <p>This is all a part of the ReCoDE Project at Imperial College London</p>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#learning-outcomes","title":"Learning Outcomes","text":"<p>Only a basic understanding of python is needed prior to beginning, with the tutorials walking you through the use of numpy and pandas to curate data for use with the hmmlearn package.</p> <ul> <li> <ol> <li>Understanding the core concepts of HMMs</li> </ol> </li> <li> <ol> <li>Curating data and training/validating your own HMM</li> </ol> </li> <li> <ol> <li>Visualising and understanding your decoded data</li> </ol> </li> </ul>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#requirements","title":"Requirements","text":"","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#academic","title":"Academic","text":"<p>A basic knowledge of python is needed.</p> <p>The tutorial will be based in numpy and pandas, two data science packages for working with and manipulating data.</p> <p>No prior knowledge of HMMs is needed, nor deep understanding of mathmatical modelling. However, if you do want to read more about HMMs, I found this resource very useful when starting out: Hidden Markov Models - Speech and Language Processing</p>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#system","title":"System","text":"Program Version Python &gt;= 3.11.0 Git &gt;= 2.43.0","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#packages","title":"Packages","text":"Package Version numpy &gt;= 1.26.4 pandas &gt;= 2.2.0 hmmlearn &gt;= 0.3.0 Matplotlib &gt;= 3.8.3 seaborn &gt;= 0.13.2 tabulate &gt;= 0.9.0 jupyter &gt;= 1.0.0","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#getting-started","title":"Getting Started","text":"","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#workflow","title":"Workflow","text":"<p>The tutorial will be taught through sequential jupyter notebooks which you will clone to your local computer. The code will be mainly written out and executed from within the notebooks so you will get a feel for the full workflow to generate and test HMMs. A few parts of the code that help the code run or tidy up the plots will be imported from elsewhere in the project.</p> <p>In the folder src there is a jupyter notebook called notebook_answers.ipynb. This notebook contains the answers to parts of the notebook where you need to write your own code.</p> <p>Once you've cloned the repo and installed the dependencies, open the first notebook as highlighted by the arrow in the structure below.</p>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#project-structure","title":"Project Structure","text":"<pre><code>.\n|\n\u251c\u2500\u2500 data\n|   \u251c\u2500\u2500 example_hmm.pkl\n|   \u251c\u2500\u2500 training_data_metadata.csv\n|   \u2514\u2500\u2500 training_data.zip\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 notebooks\n|   \u251c\u2500\u2500 1_Understanding_HMMs.ipynb &lt;---\n|   \u251c\u2500\u2500 2a_Cleaning_your_data.ipynb\n|   \u251c\u2500\u2500 2b_Training.ipynb\n|   \u251c\u2500\u2500 2c_Validating.ipynb\n|   \u2514\u2500\u2500 3_Visualising_the_results.ipynb\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 hmm_plot_functions.py\n    \u251c\u2500\u2500 misc.py\n    \u2514\u2500\u2500 notebook_answers.ipynb\n</code></pre>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#workstation","title":"Workstation","text":"<p>You'll need something to run and edit the code in the notebooks as we go along. This tutorial was created in Visual Studio Code, but you can use whatever code editor you like.</p>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#cloning-the-repository","title":"Cloning the repository","text":"<p>If you don't have it already, install git to your machine, see here for details on all OS's.</p> <p>Once installed, run the following command in the terminal after moving to the location where you want it saved. We'll then 'cd' into the created folder.</p> <pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-HMMs-for-the-discovery-of-behavioural-states.git HMM_tutorial\ncd HMM_tutorial\n</code></pre>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#setting-up-your-environment","title":"Setting up your environment","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate # with Powershell on Windows: `.venv\\Scripts\\Activate.ps1`\n</code></pre>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#install-requirements","title":"Install requirements","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>You're now ready to jump into the first notebook. Open up the notebook 1_Understanding_HMMs.ipynb.</p>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/hiddenmarkov/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Pandas","Machine Learning"]},{"location":"exemplars/idms/","title":"Bayesian Inference for SARS-CoV-2 Transmission","text":"Redirecting to exemplar docs...","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#bayesian-inference-for-sars-cov-2-transmission-modelling-using-stan","title":"Bayesian inference for SARS-CoV-2 transmission modelling, using Stan","text":"","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#author-bethan-cracknell-daniels","title":"Author: Bethan Cracknell Daniels","text":"","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#description","title":"Description","text":"<p>The aim of this exemplar is to demonstrate how to design and fit a mathematical model of disease transmission to real data, in order to estimate key epidemiological parameters and inform public health responses. Specifically, we will model the emergence of the SARS-CoV-2 variant of concern Omicron in Gauteng, South Africa. To fit the model, we use Stan, a free, accessible and efficient Bayesian inference software. Adopting a Bayesian approach to model fitting allows us to account for uncertainty, which is especially important when modelling a new pathogen or variant. The transmission model uses compartments to track the populations movement between states, for instance from susceptible to infectious. By fitting a compartmental model to epidemiological surveillance data, we will recreate the transmission dynamics of Omicron and other circulating variants, and estimate key epidemiological parameters. Together these estimates are useful for guiding policy, especially in the early stages of an emerging variant or pathogen, when there are lots of unknowns.</p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#useful-external-resources","title":"Useful external resources:","text":"<ul> <li>Bayesian workflow for disease transmission modeling in Stan</li> <li>A students guide to Bayesian statistics is accompanied by a lecture course on youtube</li> <li>A brief introduction to Stan</li> <li>The Stan manual</li> <li>Statistical Rethinking - this a thorough course on Bayesian data analysis which uses Stan. The course consists of lectures, homework and can be completed alongside reading the textbook of the same title</li> </ul>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#requirements","title":"Requirements:","text":"","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#academic","title":"Academic","text":"<p>Required: - Experience using R, for instance the Graduate school course R programming. - Knowledge of Bayesian statistics, for instance, the textbook A students guide to Bayesian statistics by Ben Lambert is a great place to start. Chapter 16 also introduces Stan.  - Download Rstan following these instructions. </p> <p>Beneficial: - Some familiarity with Stan.  - Experience of infectious disease modelling. </p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#system","title":"System","text":"Program Version R Any","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#learning-outcomes","title":"Learning outcomes","text":"<p>Upon completion of this tutorial, students will be able to:</p> <ol> <li>design an infectious disease compartmental model to answer public health questions. </li> <li>compare methods of solving ODE using Stan. </li> <li>write a Stan model to fit an infectious disease model.</li> <li>interpret Stan model diagnostics and implement appropriate solutions. </li> <li>structure R code into files based on functionality. </li> <li>write tests in R to check code. </li> </ol>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#getting-started","title":"Getting Started","text":"","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#try-the-code-with-docker-container","title":"Try the code with Docker container:","text":"<p>If you have Docker engine installed on your computer, please set Docker engine to use at least 6GB RAM, 4GB Swap, and 4 CPUs.</p> <p>Pull the container image in a command window (Windows) or a terminal window (Mac or Linux) by running </p> <p><code>docker pull jianlianggao/recode_idms:20220726</code></p> <p>When the image is pulled, to run the image in a container instance, please run the following command</p> <p><code>docker run --rm -p 127.0.0.1:8787:8787 -e DISABLE_AUTH=true jianlianggao/recode_idms:20220726</code></p> <p>Let the above command run in the terminal window and keep it open in the background, you can open <code>127.0.0.1:8787:8787</code> in your favourite web brower and you will have a RStudio (it is officially called <code>posit</code> now) interface to open and run tutorial code of chapter 1, chapter 2 or chapter 3 in R Markdown. You can modify the code in R Markdown but you do not have permissions to save the file. If you want to save changes you have made, please stop the container instance in the terminal window by pressing <code>Control + C</code>. Then start a new container instance by running</p> <p><code>docker run --rm -p 127.0.0.1:8787:8787 -v /tmp:/home/rstudio/data -e DISABLE_AUTH=true jianlianggao/recode_idms:20220703</code></p> <p>Again, please keep the command window (or terminal window) opened in the background. View RStudio from your favourite web browser by visiting 127.0.0.1:8787 and now you should be able to save changes in the /home/rstudio/data folder, which is mapping to <code>/tmp</code> in your computer. You can copy the save files to other folder later on.</p> <p>For Windows users, the mount path format <code>/tmp</code> is different, you may need to replace it with <code>d:/tmp</code> for example. This has been tested yet. We will update when we have a chance to test it on a Windows computer. In RStudio in your web browser, please run config.R before running any other code.</p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#project-structure","title":"Project structure","text":"<p>This project is split into 3 chapters:</p> <ul> <li>Chapter 1: Designing an infectious disease model</li> <li>Chapter 2: Fitting a single variant model in Stan</li> <li>Chapter 3: Fitting a multivariant model in Stan </li> </ul> <p>Each chapter is interactive, with questions and activities along the way. Therefore, for each chapter you will find 2 RMD files, one with and one without solutions. </p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#chapter-1","title":"Chapter 1","text":"<p>This chapter will demonstrate how to design an infectious disease compartmental model, including choosing the model priors and likelihood. </p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#chapter-2","title":"Chapter 2","text":"<p>This chapter will fit a single-variant compartmental model to simulated data in order to explore the transmission dynamics of omicron in Gauteng. In order to do fit the model, this chapter demonstrates how to: </p> <ol> <li>simulate data </li> <li>compare different methods of solving ordinary differential equations in Stan </li> <li>code up an infectious disease model in Stan </li> <li>fit infectious disease models in Stan </li> <li>run model diagnostics </li> <li>plot the model fit against data </li> </ol>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#chapter-3","title":"Chapter 3","text":"<p>This chapter builds on everything introduced in chapter 2 in order to fit a more complicated model. Specifically, chapter 3 shows how to fit a multistain compartmental model to simulated data in order to explore the transmission dynamics of omicron and delta in Gauteng. The model accounts for population testing, vaccination and waning immunity. Chapter 3 also includes some optional, open ended challenges. </p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#repository-structure","title":"Repository Structure","text":"<p>The documents related to each chapter are organised in the following folders:</p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#docs","title":"docs","text":"<p>This folder contains all the .md files associated with each chapter and was used to render the documentation.</p> <p>The MD file testing introduces formal testing and the package test that. All R. files with the prefix test are scripts which run formal testing on a specific function. </p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#tutorials","title":"Tutorials","text":"<p>This folder contains the corresponding .html and .Rmd files of each chapter. </p>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#r","title":"R","text":"<p>Each RMD file takes as in input multiple functions, each with a specific purpose. Frequently, these functions take as input the output of the function before. This folder contains all the functions needed to run the Rmd file. All R. files with the prefix test are scripts which run formal testing on a specific function. </p> <p>As stated above, each function has a specific purpose and the functions are designed to be run in order. The functions are as follows: </p> <ul> <li> <p>simulate_data.R: Functions to produce simulated reported incidence data using the model model1_deSolve.R or model2_deSolve.R. Takes as input parameter values for the model. Outputs a data frame of solutions to the derivatives of all compartments at each time step. </p> </li> <li> <p>calc_sim_incidence.R: Functions to calculate the simulated reported incidence. Takes as input a data frame of solutions to the derivatives of all compartments at each time step. Outputs a data frame and ggplot of reported incidence over time. </p> </li> <li> <p>run_stan_models.R: Function to fit a stan model. Uses the function draw_init_values.R. At minimum, takes as in put a list of data to fit the model and a Stan model. Outputs a fitted stan model. </p> </li> <li> <p>draw_init_values.R: Functions sourced within run_stan_models.R  which generates a different starting values for each Markov chain. Takes as input a seed value and the number of variants the model is fitting to. Outputs an initial value for each parameter and each chain. </p> </li> <li> <p>diagnose_stan_fit.R: a function to run diagnostics on a Stan fit. Takes as input a fitted  Stan model and the parameters to check. Outputs the number of divergent transitions, diagnostic plots and parameter summary statistics. </p> </li> <li> <p>plot_model_fit.R: a function to plot the results of a fitted Stan model against the data to which it was fit. Takes as input a fitted Stan model, the name of the variables to be plotted, and the simulated or observed data.</p> </li> <li> <p>compare_param_est.R: a function to compare  parameter estimates between models or methods of solving ODEs, i.e., in order to check whether a Stan model is able to recover true parameter estimates from simulated data. Takes as input a vector of true parameter values, estimated posterior mean and 95% CrI from a Stan fit and parameter names. Outputs plots comparing parameter values.  </p> </li> </ul>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/idms/#models","title":"models:","text":"<p>This folder contains the compartmental models used in part 1. </p> <p>R models:</p> <ul> <li> <p>model1_deSolve.R: a function to solve a single strain SEIQR model which is sourced by the function simulate_data.R.</p> </li> <li> <p>model2_deSolve.R: a function to solve a multistrain SEIQRS model which is sourced by the function simulate_data.R. </p> </li> </ul> <p>Stan models (written in C++):</p> <ul> <li> <p>model1_Euler_V1.stan: a Stan model of a single strain SEIQR model, the ODEs are solved using the Euler method at a single day step. </p> </li> <li> <p>model1_Euler_V2.stan: a Stan model of a single strain SEIQR model, the ODEs are solved using the Euler method at a user-defined time step.  </p> </li> <li> <p>model1_RK_V1.stan: a Stan model of a single strain SEIQR model, the ODEs are solved using the Runge-Kutta Method. </p> </li> <li> <p>model2_Euler_V1.stan: a Stan model of a multistrain SEIQRS model, the ODEs are solved using the Euler method at a user-defined time step. </p> </li> <li> <p>model2_Euler_V2.stan: a Stan model of a multistrain SEIQRS model, the ODEs are solved using the Euler method at a user-defined time step. </p> </li> </ul>","tags":["Stan","Unit Testing","Epidemiology"]},{"location":"exemplars/lattice/","title":"Modelling Organic Crystals Using a Lattice Hamiltonian","text":"Redirecting to exemplar docs...","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#modelling-organic-crystals-using-a-lattice-hamiltonian","title":"Modelling Organic Crystals Using a Lattice Hamiltonian","text":"","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#description","title":"Description","text":"<p>In this project, we use a simplified model of an organic crystal to calculate the system's excited states under illumination and their populations. We then investigate how changing the input parameters of the model changes the nature of the excited states. An example is shown in the GIF below where we demonstrate how applying an electric field changes the energies and populations of the eigenstates. </p> <p></p>","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Techniques to speed up <code>for</code> loops</li> <li>How to run <code>for</code> loops in parallel uing the <code>multiprocessing</code> package</li> <li>How to plot heatmaps using the <code>seaborn</code> package</li> </ul>","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#requirements","title":"Requirements","text":"","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#academic","title":"Academic","text":"<ul> <li>Intermediate-level python ability</li> <li>Strong, undergraduate-level understanding of quantum mechanics (recommended)</li> <li>Basic familiarity with solid state physics (recommended)</li> </ul>","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#system","title":"System","text":"<ul> <li>See the pyproject.toml file</li> </ul>","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#getting-started","title":"Getting Started","text":"<p>Start by reading through sections 1-4 which describe the physics underlying this exemplar and the structure of the code. </p> <p>Once you have been through this, you can work through the next four sections. In the first of these, we walk you through how to use the code and investigate how the eigenstates of the system change when we change the strength of the coupling between lattice sites. The next three sections focus in detail on short extracts from the code which are relevant to the learning outcomes of this exemplar. </p> <p>There are lots of things which the code can be used to do that aren't explicitly covered here! If you want a challenge, try to figure our how you could recreate the GIF shown above. Considering the eigenstates with the highest probability of being occupied, what changes about their electron-hole separation as the electric field strength is increased?</p>","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/lattice/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Parallel Computing","Visualisation","Seaborn"]},{"location":"exemplars/mandelbrot/","title":"Mathematics of the Mandlebrot Set","text":"Redirecting to exemplar docs...","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#a-c-exploration-of-the-mandelbrot-set-and-the-use-of-scientific-visualization-tools","title":"A C++ Exploration of the Mandelbrot Set and the Use of Scientific Visualization Tools","text":"","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#description","title":"Description","text":"<p>The Mandelbrot set[^1] is a well-known mathematical set, celebrated for its intricate, infinitely complex patterns that repeat at every scale. Since its popularization in the 1980s and 1990s, it has introduced new perspectives in mathematical visualization and complexity, with close links to other fields such as turbulence and computer graphics among others[^2]. This project, \"A C++ Exploration of the Mandelbrot Set and the Use of Scientific Visualization Tools\", uses the visuals of the set to introduce students to key concepts in scientific computing.</p> <p>The goal of this ReCoDe (Research and Code Development) project is to help students immerse themselves in computational methods by using the Mandelbrot set as a practical example. This approach aims to make abstract concepts more accesible and engaging. The skills acquired from this project are especially helpful for those involved in high-performance computing (HPC), grid computing, and rapid computation, allowing students to apply their knowledge to fields that require handling complex data,efficient computation and scientific visualization. <code>C++</code> is chosen for this project because it is widely used in legacy codes that PhD students need to master and adapt quickly. </p> <p> Figure 1: The main goal of the ReCoDe project is to learn how to generate a high-quality fractal image and reach higher fidelity without increasing the computation time.</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#project-overview","title":"Project Overview","text":"<p>This project is divided into three sections, which will cover the following points:</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#basic-mandelbrot-set-calculation","title":"Basic Mandelbrot Set Calculation","text":"<p>In this first section of the ReCoDe project, you will learn how to generate and perform operations on a 2D grid, transfer results directly to a high-quality picture format, and visualize them using tools like VTK. We will explore how to run iterations and inner loops efficiently using the STL (Standard Template Library) library. Additionally, you will receive guidance on measuring computation time with the <code>chrono</code> function and using a Makefile to streamline code compilation.</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#optimization-techniques","title":"Optimization Techniques","text":"<p>In this subsection, we will focus on improving the performance of the code from the first part by reducing its runtime through various multithreading paradigms and parallelization techniques all within the STL library. Specifically, we will explore using <code>std::thread</code>modifying the code to optimize its performance with these tools. We will demonstrate how these changes can reduce runtime by one-third. Additionally, you will learn how to parallelize the code for distributed-memory computer architectures, such as those used in HPC systems like CX1 and Archer2.</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#visualization-techniques","title":"Visualization Techniques","text":"<p>In this last subsection, we will focus on getting the output file in the VTK format so that we can postprocess it in ParaView and access its features. We will conclude with best practices in scientific vizualization to get a profesional rendered image.</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Efficiently manipulate large 2D arrays (representing pixels of an image) using <code>std::vector</code> to store and process image data.</li> <li>Use complex numbers for mathematical computations.</li> <li>Implement nested loops to traverse and manipulate two-dimensional data structures.</li> <li>Use of STL headers such as <code>&lt;iostream&gt;</code>, <code>&lt;complex&gt;</code>, <code>&lt;fstream&gt;</code>, <code>&lt;vector&gt;</code>, and <code>&lt;chrono&gt;</code>.</li> <li>Synchronization and Communication in MPI.</li> <li>Output <code>C++</code> computational results directly into PPM (Portable Pixmap) and VTK (Visualization Toolkit) format files for image visualization. This includes extrusion to generate a 3D file.</li> <li>Important features of scientific visualization in Paraview.</li> <li>Best practices.</li> </ul> Task Time Reading 3 hours Practising 7 hours","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#requirements","title":"Requirements","text":"<p>This project does not require prior knowledge of <code>C++</code> as there is an effot to explain each segment in detail to ensure the main ideas are clear. It is designed for readers familiar with Fortran, Matlab, or Python to learn the specifics of <code>C++</code>. The main goal is for readers to take the functions used here and apply them to their own problems.</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#system","title":"System","text":"<p>Installation instructions will be provided as needed. Except for a few cases, the project mainly uses features from the Standard Library. Some of the figures in the visualization section have been achieved using a 64BG RAM working station: however the reader can apply the same techniques on a much lower resolution image.</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#getting-started","title":"Getting Started","text":"<p>To get started, it is best to first understand how the Mandelbrot set is defined (there is a brief introduction included) and how it is implemented in the code, then move on from there.</p> <p>References</p> <p>[^1]: Mandelbrot, B. B. (1980). The Fractal Geometry of Nature. New York: W.H. Freeman and Co. [^2]: Barnsley, M. F. (1988). Fractals Everywhere. Academic Press, Inc.</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mandelbrot/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Parallel Computing","Visualisation"]},{"location":"exemplars/mcmc/","title":"Markov Chain Monte Carlo for fun and profit","text":"Redirecting to exemplar docs...    \ud83c\udfb2 \u26d3\ufe0f \ud83d\udc49 \ud83e\uddea Markov Chain Monte Carlo for fun and profit <p> Using random numbers to do all the things. </p> <p> </p>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#description","title":"Description","text":"<p>This is an exemplar project designed to showcase best practices in developing scientific software as part of the ReCoDE Project at Imperial College London.</p> <p>You do not need to know or care about Markov Chain Monte Carlo for this to be useful to you.</p> <p>Rather this project is primarily designed to showcase the tools and practices available to you when developing scientific software projects. Maybe you are a PhD student just starting, or a researcher just about to embark on a larger scale software project - there should be something interesting here for you.</p>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Creating virtual environments using Anaconda</li> <li>Plotting data using Matplotlib</li> <li>Improving code performance with <code>numba</code> and Just-in-time compilation</li> <li>Packaging Python projects into modules</li> <li>Writing a simple Monte Carlo simulation using <code>numba</code> and <code>numpy</code></li> <li>Using Test Driven Development (TDD) to test your code</li> <li>Creating unittests with <code>pytest</code></li> <li>Calculating the <code>coverage</code> of your codebase</li> <li>Visualising coarse and detailed views of the <code>coverage</code> in your codebase</li> <li>Creating property-based tests with <code>hypothesis</code></li> <li>Creating regression tests</li> <li>Using autoformatters like <code>black</code> and other development tools</li> <li>Improving performance using <code>generators</code> and <code>yield</code></li> <li>Making a reproducible Python environment using Anaconda</li> <li>Documenting your code using <code>sphinx</code></li> <li>Writing docstrings using a standardised format</li> </ul>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#requirements","title":"Requirements","text":"","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#academic","title":"Academic","text":"<p>Entry level researcher with basic knowledge of Python.</p> <p>Complementary Resources to the exemplar:</p> <ul> <li>The Turing Way has tons of great resources on the topics discussed here.</li> <li>Intermediate Research Software Development in Python</li> </ul>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#system","title":"System","text":"Program Version Python &gt;= 3.7 Anaconda &gt;= 4.1","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#getting-started","title":"Getting Started","text":"<p>Take a look at the table of contents below and see if there are any topics that might be useful to you. The actual code lives in <code>src</code> and the documentation in <code>docs/learning</code> in the form of Jupyter notebooks.</p> <p>When you're ready to dive in you have 4 options:</p>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#1-launch-the-notebooks-in-binder","title":"1. Launch the notebooks in Binder","text":"<p>NOTE: Performance might be a bit slow.</p>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#2-clone-the-repo-and-run-the-jupyter-notebooks-locally","title":"2. Clone the repo and run the Jupyter notebooks locally","text":"<pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDE_MCMCFF mcmc\ncd mcmc\npip install .[dev]\njupyter lab\n</code></pre> <p>NOTE: Better performance but requires you have Python and Jupyter installed.</p>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#3-view-the-jupyter-notebooks-non-interactively-via-the-online-documentation","title":"3. View the Jupyter notebooks non-interactively via the online documentation","text":"<p>You can read all the Jupyter notebooks online and non-interactively in the official Documentation.</p>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#4-view-the-jupyter-notebooks-non-interactively-on-github","title":"4. View the Jupyter notebooks non-interactively on GitHub","text":"<p>Click here to view the individual Jupyter notebooks.</p>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/mcmc/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 CITATION.cff # This file describes how to cite the work contained in this repository.\n\u251c\u2500\u2500 LICENSE # Outlines what legal rights you have to use this software.\n\u251c\u2500\u2500 README.md # You are here!\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 ... #Files to do with making the documentation\n\u2502   \u2514\u2500\u2500 learning\n\u2502       \u2514\u2500\u2500 #The Jupyter notebooks that form the main body of this project\n\u2502\n\u251c\u2500\u2500 pyproject.toml # Machine readable information about the MCFF package\n\u251c\u2500\u2500 readthedocs.yaml # Tells readthedocs.com how to build the documentation\n\u251c\u2500\u2500 requirements.txt # What packages MCFF requires\n\u251c\u2500\u2500 setup.cfg # Machine readable information about the MCFF package\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 MCFF # The actual code!\n\u2502\n\u2514\u2500\u2500 tests # automated tests for the code\n</code></pre>","tags":["Unit Testing","Optimisation","Statistics","Physics"]},{"location":"exemplars/neuralodes/","title":"Neural Ordinary Differential Equations","text":"Redirecting to exemplar docs...","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#neural-ordinary-differential-equations","title":"Neural Ordinary Differential Equations","text":"","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#description","title":"Description","text":"<p>This project will walk through solving Ordinary Differential Equations (ODEs) within an autograd framework (PyTorch), utilising the inbuilt tools to effectively differentiate the parameters and solutions of them, and finally incorporating Neural Networks to demonstrate how to effectively learn dynamics from data.</p>","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Writing a python module geared towards research that can be used by others</li> <li>How to take research/theoretical concepts and turn them into code</li> <li>How numerical integration works</li> <li>How neural networks work</li> <li>How neural networks and numerical integration can be combined</li> </ul> Task Time Reading 8 hours Running Notebooks 4-12 hours Practising with Own Dynamics 4+ hours","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#requirements","title":"Requirements","text":"","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#academic","title":"Academic","text":"<ul> <li>Knowledge of calculus, specifically in derivatives, integrals and limits.</li> <li>A rudimentary understanding of how floating-point/finite precision algebra works on computers.</li> <li>Basic python programming skills, knowledge of iteration, branching, etc.</li> <li>A bref understanding of vectorised computation. How CPUs/GPUs process different data in parallel</li> </ul>","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#system","title":"System","text":"<ul> <li>Python 3.10 or newer</li> <li>Poetry</li> <li>CUDA-capable GPU (for GPU training of networks)</li> </ul>","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#getting-started","title":"Getting Started","text":"","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#setting-up-python-environment","title":"Setting up Python Environment","text":"<ol> <li>Install Python 3.10 or above</li> <li>Install <code>pipx</code> following the instructions here: https://pipx.pypa.io/stable/installation/</li> <li>Install Poetry using the instructions here: https://python-poetry.org/docs/#installing-with-pipx</li> <li>Once Poetry is set up and usable, go to the root directory of this repository and run <code>poetry lock</code> followed by <code>poetry install</code>. This should install the project dependencies into a Poetry managed virtual environment.</li> <li>To run the code, use:</li> <li><code>poetry run [SCRIPT NAME]</code> to run any script in the repository.</li> <li><code>poetry shell</code> to enter a shell with the appropriate <code>python</code> and dependencies set up. From there you can use <code>python [SCRIPT NAME]</code> to run any script.</li> <li><code>poetry run jupyter notebook</code> to start a jupyter notebook in the repository environment from which the notebooks can be run.</li> <li>If using the code as a dependency (i.e. as a module that is imported in your own script), then you'll need to run <code>pip install .</code> which will install the locally available package into the current python environment.</li> </ol>","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#how-to-use-this-repository","title":"How to Use this Repository","text":"<ol> <li>Start by reading <code>Chapter 1 - Introduction to Ordinary Differential Equations (ODEs)</code> and refer to the introductory notebooks for the implementation of the concepts.</li> <li>Study the jupyter notebooks on the implementations in further detail: [Fill with notebook names for introductory material]</li> <li>Study Chapter 2 for a walk-through of the module structure</li> <li>Study jupyter notebooks for training scripts as well as visualisation of results</li> </ol>","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 neuralode\n\u2502   \u251c\u2500\u2500 integrators\n\u2502   \u251c\u2500\u2500 models\n\u2502   \u251c\u2500\u2500 plot\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 notebooks\n|   \u251c\u2500\u2500 01-simple-integration-routines.ipynb\n|   \u251c\u2500\u2500 02-arbitrary-adaptive-tableaus.ipynb\n|   \u251c\u2500\u2500 03-the-adjoint-method.ipynb\n|   \u251c\u2500\u2500 04-driven-harmonic-oscillator.ipynb\n|   \u2514\u2500\u2500 05-the-inverted-pendulum.ipynb\n\u251c\u2500\u2500 docs\n|   \u2514\u2500\u2500 01-introduction.md\n|   ...\n</code></pre>","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/neuralodes/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Machine Learning","PyTorch","Ordinary Differential Equations","Nonlinear Feedback Control","Optimisation"]},{"location":"exemplars/nsprop/","title":"Navier-Stokes Propagator","text":"Redirecting to exemplar docs...","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#navier-stokes-propagator","title":"Navier-Stokes Propagator","text":"","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#description","title":"Description","text":"<p>The repository contains source files for ReCoDE-NavierStokesPropagator.jl, a Julia package written to solve the chaotic Navier-Stokes equations. The project aims to exemplify the improvement of simulation code quality using good programming abstractions (Julia Structs) and defining sensible simulation constructs that reflect each core component of the simulation. Additionally, the repository adopts good programming practices by implementing detailed documentation with unit tests written for each simulation construct.</p> <p>The algorithm used here is inspired from Diablo, written by John R. Taylor, with minor modifications to the mesh spacing.</p>","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Ability to distinguish and compartmentalise a multi-facet simulation algorithm into sensible simulation constructs via the implementation with Julia Structs </li> <li>Adopting good testing methodologies, through implementation of unit and integration tests</li> <li>Understanding the FFTW.jl and HDF5.jl libraries and their abstraction through the usage of utility modules in their respective source files</li> </ul> Task Time Reading 3 hours","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#requirements","title":"Requirements","text":"","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#theoretical","title":"Theoretical","text":"<ul> <li>Exploring Fourier series and its relation to the discrete Fourier transform</li> <li>Understanding of the finite difference method for approximating spatial derivatives and the use of implicit/explicit time-stepping schemes</li> <li>Familiarity with Julia. The Early Career Researcher Institute also provides an Introduction to Julia course.</li> </ul> <p>Note : The theoretical understanding of the finite difference method, Fourier series and the time-stepping scheme are not a major necessity. An understanding of the mathematical structure for algorithimic implementation is sufficient to understand the ideas discussed in this tutorial.</p>","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#system","title":"System","text":"<ul> <li>Install Julia on your system.</li> <li>Install VS Code on your system.</li> <li>Install Julia language extension on VS Code.</li> </ul>","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#getting-started","title":"Getting Started","text":"<ol> <li>Clone the repository using <code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-NavierStokesPropagator.git</code> and enter the repository directory via <code>cd ReCoDE-NavierStokesPropagator</code>.</li> <li>Open the system terminal via VSCode and launch the Julia REPL via <code>julia</code>.</li> <li>Launch Julia <code>pkg</code> mode via pressing <code>]</code> in the Julia REPL.</li> <li>In the <code>pkg</code> mode, activate the package environment via <code>activate .</code>.</li> <li>Testing can be done by entering <code>test</code> in the REPL's pkg mode.</li> </ol>","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 NavierStokesPropagators.jl\n\u2502   \u251c\u2500\u2500 NavierStokesPropagatorsCore.jl\n\u2502   \u251c\u2500\u2500 sim\n\u2502   \u2502   \u251c\u2500\u2500 DomainDescriptors.jl\n\u2502   \u2502   \u251c\u2500\u2500 SimulationConditions.jl\n\u2502   \u2502   \u2514\u2500\u2500 States.jl\n\u2502   \u2514\u2500\u2500 util\n\u2502       \u251c\u2500\u2500 InputOutputManagers.jl\n\u2502       \u251c\u2500\u2500 LinearSolvers.jl\n\u2502       \u2514\u2500\u2500 Transformations.jl\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 sim (mirrors src/sim)\n\u2502   \u251c\u2500\u2500 util (mirrors src/util)\n\u2502   \u251c\u2500\u2500 StateTransformations.jl\n\u2502   \u2514\u2500\u2500 runtest.jl\n\u251c\u2500\u2500 docs\n\u2514\u2500\u2500 .github\n    \u2514\u2500\u2500 (.yml files for CI/CD)\n</code></pre>","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/nsprop/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license.</p>","tags":["Object-Oriented Programming","Unit Testing"]},{"location":"exemplars/penalisedreg/","title":"Predicting Colitis with Penalised Regression","text":"Redirecting to exemplar docs...","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#predicting-colitis-using-penalised-regression","title":"Predicting colitis using penalised regression","text":"","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#description","title":"Description","text":"<p>The field of predictive medicine is becoming increasingly popular. A key challenge is dealing with the high-dimensionality of genetics, where the number of genes (factors) is far larger than the number of patients (observations), resulting in classical statistical methods breaking down. This has lead to the rise of penalised regression, where a penalty is applied to the factors to induce sparsity, so that a large majority of factors are deemed irrelevant, allowing for statistical inference to occur. One popular penalised approach, covered in most undergraduate and graduate statistics modules, is the lasso. The lasso has gained a large amount of traction over the last 20 years, with many extensions also proposed. </p> <p>The lasso has found particular use in genetics, as it is computationally efficient and is able to select relevant genes as being associated with a disease. One particular useful extension is the group lasso, which can apply this penalisation onto groups of variables. As genes are naturally found in groups (pathways), this extension has also found extensive use in genetics. One final approach is the sparse-group lasso, which combines the two. This project shows how to apply these three methods to predicting whether a patient has colitis, an inflammatory bowel disease. </p>","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#learning-outcomes","title":"Learning Outcomes","text":"<p>Main outcomes:</p> <ul> <li>Go through an end-to-end analysis of how we can use penalised regression models to predict cases of colitis using gene expression data.</li> <li>Gain an insight into how genetics data can be downloaded, cleaned, and prepared for use in analysis \u2013 this gives a good insight into how general R data manipulation works.</li> <li>Follow the introduction into predictive modelling by showing how a fitted model can be used to form predictions.</li> <li>Understand how the wide class of penalised regression models work and how they can be implemented in R. This will include mathematical and statistical background to how these methods work, which will touch upon important regression topics that form the foundation for most models used widely in academia and industry.</li> </ul> <p>Optional outcomes:</p> <ul> <li>The example provided is for predicting colitis data. The questions provided in each section will provide guidance through another example, where breast cancer is predicted. Therefore, the questions will form a comprehensive additional analysis of breast cancer data.</li> <li>An additional class of models, SLOPE models, are presented in the optional sections of the chapters. SLOPE models are adaptive versions of the models covered in the main outcomes and provide additional insight into the direction of the current research in this area.</li> </ul> Task Time Chapter 1 2 hours Chapter 2 5 hours Chapter 3 4 hours","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#requirements","title":"Requirements","text":"","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#academic","title":"Academic","text":"<ul> <li>Basic understanding of mathematical concepts that underpin penalised regression: linear algebra, matrices, geometry.</li> <li>Basic statistics knowledge, including linear regression and model fitting.</li> <li>Genetics background is not needed. Basic background information is provided in Chapter 1.</li> <li>Familiarity with R programming language.</li> </ul>","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#system","title":"System","text":"Program Version R Any","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#getting-started","title":"Getting Started","text":"<p>The chapters are structured to be worked through sequentially. Each chapter contains optional content that adds extra insight into the problem but is not required to solve to the core problem. </p>","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/penalisedreg/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Statistics","Logistic Regression"]},{"location":"exemplars/perceptions/","title":"PyTorch for end-to-end deep learning","text":"Redirecting to exemplar docs...","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#pytorch-implementation-for-end-to-end-training-of-a-deep-learning-model","title":"PyTorch implementation for end-to-end training of a deep learning model","text":"","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#description","title":"Description","text":"<p>Recode Perceptions is a PyTorch implementation of a deep convolutional neural network model trained on Places365 data, developed by Emily Muller.</p> <p>This model is trained on a subset of 100K images which have outcome labels that are associated to factors which are relevant for environmental health.</p>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Be aware of different types of Computer Vision tasks</li> <li>Load an image dataset in PyTorch</li> <li>Be able to explain what a convolutional layer does and how it's different from a fully-connected layer</li> <li>Identify different components of a CNN</li> <li>Load a pre-trained model in PyTorch</li> <li>Be able to use PyTorch to train a model on a dataset</li> <li>Iterate on design choices for model training</li> </ul>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#requirements","title":"Requirements","text":"","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#academic","title":"Academic","text":"","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#system","title":"System","text":"Program Version Python &gt;= 3.7 Anaconda &gt;= 4.1 Access to Imperial's HPC (optional) Last updated: 22-07-2022","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#getting-started","title":"Getting Started","text":"","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#how-to-use-this-repository","title":"How to Use this Repository","text":"<p>This repository has 3 core learning components:</p> Title Description Location Key Learning Objectives Introduction to Environmental Health and Imagery This is a short video, introducing the domain, methods and describing some pioneering work in this field Video Introduction to the field. Understand different methods. Understand different types of data. Be aware of seminal research. Foundations of Deep CNN's using PyTorch A Jupyter Notebook familiarising students with core components of deep learning framework using PyTorch. Jupyter Notebook Be aware of different types of Computer Vision tasks. Be able to explain what a convolutional layer does and how it's different from a fully-connected layer. Identify different components of a CNN. Load an image dataset in PyTorch. Load a pre-trained model in PyTorch. Be able to use PyTorch to train a model on a dataset <code>deep_cnn</code> This module contains all the code needed to fine-tune a deep neural network on the Places365 classification task. Detailed documentation is provided in the folder README.md but requires to be set up (below). deep_cnn Use terminal for executing python scripts Train a PyTorch model and visualise results. Export training to the HPC. Implement bach script Iterate on model hyperparameters to optimise model. Prediction and Interpretability using Object Detections In the final analysis, the pretrained network from <code>deep_cnn</code> is used to predict on the test set. Explainable features, specifically Object Detections are extracted from the images and correlated to each scene category. Jupyter Notebook Run inference using a pre-trained model. Explore Tensorflow DeepLab API model. <p>The suggested way to use this repository is as follows:</p> <ul> <li>Continue with set-up as detailed below.</li> <li>Complete learning materials 1 (Video) and 2 (Jupyter Notebook).</li> <li>Continue to model training in 3 (deep_cnn).</li> <li>Finally complete inference using pre-trained model from the previous step in 4 (Jupyter Notebook)</li> </ul>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#getting-started_1","title":"Getting started","text":"<p>Clone this repository into your local drive.</p> <pre><code>git clone https://github.com/ImperialCollegeLondon/recode-perceptions.git\ncd recode-perceptions\n</code></pre>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#setting-up-a-virtual-environment","title":"Setting up a virtual environment","text":"<p>We will set up a virtual environment for running our scripts. In this case, installing specific package versions will not interfere with other programmes we run locally as the environment is contained. Initially, let's set up a virtual environment:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>This will create a new folder for the virtual environment named <code>perceptions</code> in your repository. We activate this environment by running</p> <pre><code>conda activate perceptions\n</code></pre> <p>All the dependencies are installed along with the virtual environment. We will manually install the development tools since we do not need those dependencies when we export to HPC and create a virtual environment there.</p>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#setting-up-the-development-virtual-environment","title":"Setting up the development virtual environment","text":"<p>The <code>pytest</code> and pre-commit module is required for running tests and formatting. This can be installed by running:</p> <pre><code>conda install --file requirements-dev.txt\n</code></pre> <p>Now run the tests below to make sure everything is set up correctly. Then, proceed to the video.</p>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#testing","title":"Testing","text":"<p>To run all tests, install <code>pytest</code>. After installing, run</p> <pre><code>pytest tests/ -v\n</code></pre>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/perceptions/#project-structure","title":"Project Structure","text":"<pre><code>recode-perceptions\n\u2502   README.md\n\u2502   .pre-commit-config.yaml             # pre-commit options file\n\u2502   setup.cfg                           # set-up for pre-commit\n\u2502   requirements-dev.txt                # python packages for development\n\u2502   requirements.txt                    # python packages for running programme\n\u2502   environment.sh                      # HPC environment set-up (see)\n\u2502   submit.pbs                          # job submission for HPC (see)\n\u2502\n\u2514\u2500\u2500\u2500deep_cnn                            # module for model training (see)\n\u2502   \u2502   __init__.py\n\u2502   \u2502   __main__.py\n\u2502   \u2502   logger.py\n\u2502   \u2502   utils.py\n\u2502   \u2502   dataset_generator.py\n\u2502   \u2502   model_builder.py\n\u2502   \u2502   train.py\n\u2502   \u2502   train_model.py\n|\n\u2514\u2500\u2500\u2500docs                                # learning materials\n\u2502   \u2502   1-cnn-intro.ipynb\n|   \u2502   2-cnn-training.md\n\u2502\n\u2514\u2500\u2500\u2500input                               # folder to download images\n\u2502   \u2502   keep.txt                        # images to remove\n\u2502   \u2514\u2500\u2500\u2500places365standard_easyformat    # images downloaded (see)\n\u2502   \u2502   \u2502   ...                         # metadata\n\u2502   \u2502   \u2514\u2500\u2500\u2500places365_standard\n\u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500train\n\u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500val\n\u2502\n\u2514\u2500\u2500\u2500outputs\n\u2502   \u2514\u2500\u2500\u2500logger                          # logging output from model training\n\u2502   \u2514\u2500\u2500\u2500models                          # save model checkpoints\n\u2502   \u2514\u2500\u2500\u2500results                         # save model training metrics\n\u2502\n\u2514\u2500\u2500\u2500tests                               # folder for testing\n\u2502   \u2514\u2500\u2500\u2500places_test_input\n\u2502   \u2502   ...\n</code></pre>","tags":["pyTorch","Tensorflow","Computer Vision","Machine Learning","HPC"]},{"location":"exemplars/permeation/","title":"Time Lag Analysis","text":"Redirecting to exemplar docs...","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#time-lag-analysis-application","title":"Time Lag Analysis Application","text":"<p>This application provides a user-friendly interface for analysing gas permeation data using the time lag method. Load data from Excel files in <code>data</code>, specify your experimental setup, run the analysis, and visualise the results.</p> <p></p> <p>This exemplar was developed at Imperial College London by Louis Nguyen in collaboration with Dr. Diego Alonso Alvarez from Research Software Engineering and Dr. Chris Cooling from Research Computing &amp; Data Science at the Early Career Researcher Institute.</p>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#learning-outcomes","title":"Learning Outcomes \ud83c\udf93","text":"<p>By using this application, you will:</p> <ul> <li>Gain an understanding of time lag analysis in gas permeation experiments.</li> <li>Learn how to prepare and analyse your experimental data.</li> <li>Get hands-on experience with a GUI for scientific data analysis.</li> <li>Be able to adapt the application to your specific research interests.</li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#target-audience","title":"Target Audience \ud83c\udfaf","text":"<p>This exemplar is aimed at researchers, students, and engineers in fields such as Chemical Engineering, Materials Science, and Polymer Science who work with gas or vapor permeation through membranes and films. It is suitable for those who need to:</p> <ul> <li>Analyse experimental permeation data.</li> <li>Understand the principles of the time-lag method.</li> <li>Learn how to implement data analysis workflows in Python.</li> <li>Utilise a graphical user interface for scientific computation.</li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#disciplinary-background","title":"Disciplinary Background \ud83d\udd2c","text":"<p>The time-lag method is a widely used experimental technique to determine the diffusion and permeability coefficients of gases or vapours in polymeric membranes. This information is crucial for designing and optimising materials for various applications, such as gas separation, packaging, and protective coatings. This exemplar provides a practical tool and learning resource for performing time-lag analysis, bridging the gap between experimental data acquisition and material property characterisation. It can be useful for researchers in materials science, chemical engineering, polymer chemistry, and any field involving the study of transport phenomena in materials.</p>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#prerequisites","title":"Prerequisites \u2705","text":"","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#academic","title":"Academic \ud83d\udcda","text":"<ul> <li>Basic understanding of mass transfer principles and gas permeation in materials.</li> <li>Familiarity with data handling (e.g., using spreadsheets like Excel).</li> <li>Some experience with Python programming is beneficial for extending the application or understanding the source code, but not strictly required for using the GUI.</li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#system","title":"System \ud83d\udcbb","text":"<ul> <li>Anaconda or Miniconda installed on your system.</li> <li>Git (optional, for cloning the repository).</li> <li>Python 3.12+.</li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#software-tools","title":"Software Tools \ud83d\udee0\ufe0f","text":"<ul> <li>Programming language: Python</li> <li>Core libraries:<ul> <li>NumPy: For numerical operations.</li> <li>Pandas: For data manipulation and analysis, especially with Excel files.</li> <li>SciPy: For scientific and technical computing, including curve fitting and optimisation.</li> <li>Matplotlib: For creating static, interactive, and animated visualisations.</li> </ul> </li> <li>GUI: CustomTkinter</li> <li>Environment management: Conda</li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#getting-started","title":"Getting Started \ud83d\ude80","text":"<ol> <li> <p>Clone or download the repository:</p> <p><pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDe-permeation-analysis-app\ncd ReCoDe-permeation-analysis-app\n</code></pre> 2.  Create and activate the <code>permeation-env</code> conda environment:</p> <p><pre><code>conda env create -f environment.yml\nconda activate permeation-env\n</code></pre> 3.  Run the application:</p> <pre><code>python src/app.py\n</code></pre> </li> </ol> <p>The application interface allows you to:</p> <ol> <li>Load data: Use the provided example data or upload your own Excel files from the <code>data</code> directory.</li> <li>Specify parameters: Input your experimental setup details.</li> <li>Run analysis: Click the <code>Run Analysis</code> button to process the data.</li> <li>View results:<ul> <li>Numerical results will appear in the designated text box.</li> <li>Visualisations (plots) will be displayed on the right-hand panel.</li> <li>Save plots using the \"Save\" button located at the top right of the plot area.</li> </ul> </li> </ol>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#data","title":"Data \ud83d\udcca","text":"<p>Example datasets are provided in the <code>data/</code> directory. These are Excel files (<code>.xlsx</code>) representing typical raw data from gas permeation experiments.</p> <ul> <li>Licensing: The provided data may be synthetic or anonymised for demonstration purposes within this exemplar. This data remains the property of the original owner, and permission must be sought for any use outside of this exemplar.</li> <li>Location: Included directly in the repository under the <code>data/</code> folder.</li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#documentation-guide","title":"Documentation Guide","text":"<ol> <li>Explore the documentation in the <code>docs</code> folder, starting with <code>01-Home.md</code> and progressing through the guides.</li> <li>Work through the exercises in <code>09-Exercises-and-Best-Practices.md</code>.</li> </ol>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#estimated-time","title":"Estimated Time \u23f3","text":"Task Time Introduction 20 mins Understanding Theoretical Background 30 mins Data Management and Processing 25 mins Time Lag Analysis Implementation 25 mins Python PDE Implementation 25 mins Visualisation Techniques 20 mins GUI Implementation 15 mins Application Workflow 20 mins Subtotal: Core Documentation 3 hours Exercises and Best Practices 3 hours Total Estimated Time 6 hours","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#project-structure","title":"Project Structure \ud83d\uddc2\ufe0f","text":"<pre><code>.\n\u251c\u2500\u2500 data/                      # Example Excel data files for demonstration\n\u251c\u2500\u2500 docs/                      # Markdown documentation and guides\n\u2502   \u251c\u2500\u2500 assets/                # Images and other assets for documentation\n\u2502   \u251c\u2500\u2500 01-Home.md\n\u2502   \u251c\u2500\u2500 02-Theoretical-Background.md\n\u2502   \u251c\u2500\u2500 03-Data-Management-and-Processing.md\n\u2502   \u251c\u2500\u2500 04-TimelagAnalysis-Implementation.md\n\u2502   \u251c\u2500\u2500 05-Python-PDE-Implementation.md\n\u2502   \u251c\u2500\u2500 06-Visualisation.md\n\u2502   \u251c\u2500\u2500 07-GUI-Implementation.md\n\u2502   \u251c\u2500\u2500 08-Application-Workflow.md\n\u2502   \u251c\u2500\u2500 09-Exercises-and-Best-Practices.md\n\u2502   \u2514\u2500\u2500 index.md               # Main index for MkDocs\n\u251c\u2500\u2500 notebooks/                 # (Currently empty, could be used for Jupyter notebooks)\n\u251c\u2500\u2500 src/                       # Source code for the application\n\u2502   \u251c\u2500\u2500 __init__.py            # Initialises the src package\n\u2502   \u251c\u2500\u2500 app.py                 # Main application file (GUI implementation)\n\u2502   \u251c\u2500\u2500 calculations.py        # Functions for time lag calculations\n\u2502   \u251c\u2500\u2500 data_processing.py     # Functions for loading and preprocessing data\n\u2502   \u251c\u2500\u2500 time_lag_analysis.py   # Workflow for performing time lag analysis\n\u2502   \u251c\u2500\u2500 util.py                # Utility functions and plot styling\n\u2502   \u2514\u2500\u2500 visualisation.py       # Functions for creating plots\n\u251c\u2500\u2500 tests/                     # (Planned, to house test scripts)\n\u251c\u2500\u2500 environment.yml            # Conda environment specification\n\u251c\u2500\u2500 LICENSE.md                 # Project license\n\u251c\u2500\u2500 mkdocs.yml                 # Configuration for MkDocs\n\u251c\u2500\u2500 README.md                  # This file\n\u251c\u2500\u2500 requirements-dev.txt       # Development dependencies (e.g., for MkDocs)\n\u2514\u2500\u2500 requirements.txt           # Core Python package dependencies\n</code></pre> <p>Code is organised into logical components:</p> <ul> <li><code>data</code>: Contains example experimental data files.</li> <li><code>docs</code>: Houses all user documentation, guides, and supporting images.</li> <li><code>src</code>: Contains the core Python scripts for the application's logic, calculations, data processing, and GUI.</li> <li><code>notebooks</code>: Reserved for potential Jupyter Notebooks for interactive exploration or tutorials.</li> <li><code>tests</code>: Reserved for future test scripts.</li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#best-practice-notes","title":"Best Practice Notes \ud83d\udcdd","text":"<ul> <li>Reproducible environments: The <code>environment.yml</code> file ensures that users can create an identical Conda environment with all necessary dependencies.</li> <li>Modular code: The source code in <code>src/</code> is organised into modules with specific responsibilities (e.g., <code>data_processing.py</code>, <code>calculations.py</code>, <code>visualisation.py</code>, <code>app.py</code>).</li> <li>Documentation: Comprehensive documentation is provided in the <code>docs/</code> folder, built with MkDocs.</li> <li>Code comments: The Python scripts include comments to explain the logic.</li> <li>User-friendly GUI: The application features a GUI to make the analysis accessible to users without extensive programming knowledge.</li> <li>Future:<ul> <li>Code testing: Implementing unit and integration tests in the <code>tests/</code> directory.</li> <li>Continuous Integration (CI): Setting up CI pipelines for automated testing.</li> </ul> </li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#additional-resources","title":"Additional Resources \ud83d\udd17","text":"<ul> <li>The <code>docs/</code> folder within this repository contains detailed guides and explanations.</li> <li>For specific Python libraries, refer to their official documentation:<ul> <li>NumPy</li> <li>Pandas</li> <li>SciPy</li> <li>Matplotlib</li> <li>CustomTkinter (built on Tkinter).</li> </ul> </li> </ul>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/permeation/#license","title":"License \ud83d\udcc4","text":"<p>This project is licensed under the BSD-3-Clause license.</p>","tags":["GUI","Partial Differential Equations","Diffusion"]},{"location":"exemplars/pythongui/","title":"Multi-channel Python GUI","text":"Redirecting to exemplar docs...","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#recode_pythongui","title":"ReCoDE_PythonGUI","text":"","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#a-multi-channel-gui-for-real-time-data-display-and-analysis","title":"A multi-channel GUI for real-time data display and analysis","text":"<p>In this course, you will learn how to create a GUI program to display and analyze data in real-time with Python. The GUI program is designed to display and analyze data from a file. The GUI program is also designed to display data from a data acquisition system through serial port communication. The GUI program is developed using dearpygui, a GPU-based Python GUI framework. </p> <p>By the end of this course, you will be able to design a GUI program looks like the above picture, which can display data from a file or a data acquisition system in real-time. You will also be able to select the file to display, select the channels to display, change the color of the lines, and start, stop, and pause the display. You will also learn how to add data analysis functions to the GUI program and display analysis results in real-time.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#before-we-start","title":"Before we start","text":"<p>Here is some useful links from Diego Alonso \u00c1lvarez telling you how important GUI is for research software. - GUIs for research software: Why are they relevant? (part one) - GUIs for research software: Why are they relevant? (part two)</p> <p>The GUI framework we use in this course is dearpygui, here is a link to the domentation of dearpygui Dearpygui documentation. Dearpygui is chosen for its ability to create a GUI with a few lines of code. It is also a very powerful GUI framework that can be used to create a complex GUI. Most importantly, it supports GPU rendering and multi-threading, which makes it very fast and responsive.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#introduction-to-gui-design-for-real-time-data-display-and-analysis","title":"Introduction to GUI design for real-time data display and analysis","text":"","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#what-is-gui","title":"What is GUI?","text":"<p>GUI stands for Graphical User Interface. It is a type of user interface that allows users to interact with electronic devices through graphical icons and visual indicators. GUIs were introduced in reaction to the perceived steep learning curve of command-line interfaces (CLIs), which require commands to be typed on the keyboard. The actions in a GUI are usually performed through direct manipulation of the graphical elements. The graphical elements include but not limited to windows, menus, buttons, scrollbars, icons, etc.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#why-do-we-need-gui","title":"Why do we need GUI?","text":"<p>GUIs are widely used in many applications including operating systems, web browsers, office suites, email programs, etc. GUIs are also used in scientific applications, including data acquisition, data analysis, and data visualization. Their utilization in scientific applications stems from their user-friendly nature, facilitating the creation of intuitive interfaces tailored to scientific tasks.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#gui-frameworks","title":"GUI frameworks","text":"<p>There are many GUI frameworks available for Python. Some of the popular GUI frameworks include Tkinter, PyQt, PySide, wxPython, Kivy and so on. Different GUI frameworks have different ways to create a GUI. In this course, we will use dearpygui as the GUI framework. Dearpygui is chosen for its ability to create a GUI with a few lines of code. It is also a very powerful GUI framework that can be used to create a complex GUI. Most importantly, it supports GPU rendering and multi-threading, which makes it very fast and responsive.</p> <p>There are plenty of GUI software developed by dearpygui. You can find them in the dearpygui showcase.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#structure-of-this-course","title":"Structure of this course","text":"<p>Day 1: Develop familiarity with frameworks and key components for GUI design.  </p> <ul> <li> <p>Contents:</p> <p>You will be provided with sample codes to familiarize commonly used GUI widgets. You can design whatever you want using provided widgets. (Sample data will be provided) * Outcomes:</p> <p>You will be able to design a GUI with different kinds of widgets including buttons, text boxes, figures and so on.</p> </li> </ul> <p>Day 2: Design a GUI to dynamically display data with single channel.</p> <ul> <li> <p>Contents:  </p> <p>You will be provided a template with sample modularized codes to illustrate how to add a figure on GUI and make it work. You will also learn how to display data from a file with single channel.</p> </li> <li> <p>Outcomes:</p> <p>You will learn how to update data in a plot widget and be able to design a GUI to display data with single channel.</p> </li> </ul> <p>Day 3: Design a GUI to dynamically display data with multiple channels.</p> <ul> <li> <p>Contents:     You will extend the work in step 2 by referring to another template provided to add two more channles. You will learn how to add multiple figures on GUI and make them work. You will also learn how to display data from a file with multiple channels.</p> </li> <li> <p>Outcomes:</p> <p>You will gain an understanding of tags in dearpygui and be able to update different widgets according to their tags. You will also be able to design a GUI to display data with multiple channels. </p> </li> </ul> <p>Day 4: Design a GUI with control panel to control the display.</p> <ul> <li> <p>Contents:</p> <p>You will be provided a new template with modularized functions and control widgets as well as directive instructions for adding control widgets used in GUI design. Four control functions will be provided as examples, including select file to display, start, stop and pause the display, select channels to be displayed, and change the color of lines.</p> </li> <li> <p>Outcomes:</p> <p>You will have an understanding of the interactions between different widgets, taking inputs from users and updating widgets accordingly.</p> </li> </ul> <p>Day 5: Extension: Serial Port Communication</p> <ul> <li> <p>Contents:</p> <p>You will be provided a new template with modularized functions and control widgets to display data from a data acquisition system through serial port communication. You will learn how to display data from a real-time data acquisition system with serial communication.</p> </li> <li> <p>Outcomes:</p> <p>You will gain basic and necessary knowledge on how to use serial port communication with Python. You will also be able to design a GUI that displays data from a data acquisition system in real-time though serial port communication. </p> </li> </ul>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#preparations","title":"Preparations","text":"","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#software","title":"Software","text":"<ul> <li>Python 3.9.7</li> <li>VisualStudio Code (newest version)</li> </ul> <p>Python packages that need to be installed will be introduced in corresponding sections.</p> <p>You will use virual serial port emulator to and a serial port monitor for Day 5. An instruction will be provided in Day 5.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#hardware","title":"Hardware","text":"<ul> <li>A computer with Windows 10 or Windows 11 operating system</li> </ul> <p>You can use a computer with Mac/Linux operating system. However, the virtual serial port emulator for day 5 only supports Windows operating system.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#data","title":"Data","text":"<p>We are going to use data from the CHARIS database. The CHARIS database contains multi-channel recordings of ECG, arterial blood pressure (ABP), and intracranial pressure (ICP) of patients diagnosed with traumatic brain injury (TBI).</p> <p>As the full dataset is huge, we will only use two of them in this course. Data are stored in <code>Data</code> folder. You should put the data file you want to read in this folder. You can then access the data file by setting the path to <code>Data</code> folder. For example, you can use the following code to read the data file <code>charis4.dat</code>. <pre><code>import numpy as np\nfilename = 'Data/charis4.dat'\ndatafile =open(filename, 'rb')\ndtype = np.dtype('int16')\ndata = np.fromfile(datafile,dtype)\n</code></pre></p> <p>The full dataset can be downloaded from CHARIS database. </p>","tags":["NumPy","GUI"]},{"location":"exemplars/pythongui/#best-practice-notes","title":"Best practice notes","text":"<p>Debugging GUI codes is different from traditional code. </p> <ul> <li> <p>Have the structure of GUI program in mind. You are placing widgets on a planar canvas. This helps to find missing widgets as they can be blocked by others and determine the relative location of the widgets. </p> </li> <li> <p>Always keep in mind that there still exists CLI when you design GUI, try to print essential variables for debugging. </p> </li> <li> <p>Unless you enable multithreading, your codes run in serial. If your data processing codes are time-consuming, the GUI will be blocked.</p> </li> <li> <p>Assign meaningful names to each widget. For example, you can name a button as <code>start_button</code>, a text box as <code>file_path_textbox</code>, and a figure as <code>data_figure</code>. This helps to find the widget you want to update. </p> </li> <li> <p>Compared with displaying data statically, you should have an understanding of sliding window when you dynamically display data. </p> </li> </ul> <p>Always assume the users know nothing about how to use your program. For example, they can start running the progran without selecting a file. You can avoid this by disable the start button unless a file is selected.</p>","tags":["NumPy","GUI"]},{"location":"exemplars/rnaseq/","title":"RNA Sequencing Analysis","text":"Redirecting to exemplar docs...","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#rna-seq-analysis","title":"RNA-seq Analysis","text":"","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#author-jack-gisby","title":"Author: Jack Gisby","text":"","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#description","title":"Description","text":"<p>The RNA-seq analysis exemplar involves the development of a pipeline for processing large volumes of biological data. The development of next generation sequencing technologies has facilitated a systems-level approach to biological and biomedical research. In particular, RNA sequencing (RNA-seq) has become a ubiquitous method for gene expression profiling. However, the colossal datasets produced by this method pose a new challenge for life science researchers, who commonly have little statistical or computational training. The processing of sequencing data commonly requires the development of custom workflows that can run in parallel on university computing clusters. Furthermore, the quality control and statistical analysis of these data requires specialist knowledge and purpose-built software packages.</p> <p>This project demonstrates the development of a pipeline for processing RNA-seq datasets on the computing clusters, such as the Imperial College Research Computing Service (RCS), and basic statistical analysis of the normalised data. This will involve:</p> <ul> <li>Quality control and trimming of raw RNA-seq reads</li> <li>Alignment of reads to the human reference genome</li> <li>Conversion of aligned reads to a matrix of gene counts</li> <li>Downstream statistical analysis:</li> <li>Data normalisation</li> <li>Unsupervised analysis (e.g. PCA)</li> <li>Differential expression and enrichment using edgeR</li> </ul>","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#requirements","title":"Requirements","text":"","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#academic","title":"Academic","text":"<ul> <li>Familiarity with bash (A course such as \"The Linux Command Line for Scientific Computing\", hosted by the Imperial Research Computing &amp; Data Science Team, would be provide a suitable background.)</li> <li>Familiarity with R programming language</li> <li>Familiarity with computing clusters</li> </ul>","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#system","title":"System","text":"Program Version R Any Anaconda &gt;=4.13.0 Access to Imperial's HPC Last updated: 29-07-2022","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#learning-outcomes","title":"Learning Outcomes","text":"<p>Upon completion of this tutorial, students will be able to:</p> <ol> <li>parallelise bioinformatics tools on computing clusters, such as the Imperial RCS</li> <li>develop a reproducible pipeline</li> </ol> <p>Tools used achieve this as part of the exemplar include:</p> <ul> <li>Nextflow, a workflow management system that makes it easy to develop data-driven pipelines.</li> <li>Conda, an package management system that allows you to share your local environment with others.</li> <li>Docker, an application for packaging dependencies into a virtual container.</li> <li>Git/GitHub, a version control system that integrates with nextflow to make pipelines shareable.</li> <li>Continous integration, a practice of automatic code testing.</li> </ul>","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#getting-started","title":"Getting started","text":"<p>To get started using the pipeline, install Nextflow and either of Docker, Singularity or Conda. Then, the following command can be used to run the pipeline locally using a small test dataset:</p> <pre><code>nextflow run -r main ImperialCollegeLondon/ReCoDE_rnaseq_pipeline -profile test,docker\n</code></pre> <p>Note that this pipeline is not designed to handle all types of RNA-seq data (e.g. it was not designed for paired-read data). If you have large amounts of RNA-seq data to process, we recommend using the nextflow-core RNA-seq pipeline.</p> <p>To download the pipelines, install Git and clone this repository from the command line, using the following code:</p> <pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDE_rnaseq_pipeline.git\n</code></pre> <p>To run the first two pipelines, you will need to install the command line applications listed in <code>environment.yml</code>. Alternatively, you can use conda to install these applications, using the following code:</p> <pre><code>conda env create -f environment.yml\nconda activate recode_rnaseq\n</code></pre> <p>This conda environment also contains Nextflow, which we will be using to create the final iteration of the data processing pipeline. To run the nextflow pipeline, you will also need either Docker or Conda installed.</p> <p>The scripts in this repository are designed for use with the Imperial computing cluster. If you do not have access to the cluster, you might be able to adapt the code to your own cluster's configuration. Alternatively, the primary pipeline uses nextflow, which is adaptable to many different platforms. You could run the nextflow pipeline on your local computer, or configure it to run on another cluster or even the cloud.</p> <p>Some of these tools are not available on windows. If you wish to run the basic pipeline discussed in this document but you use Windows, you could use the Windows Subsystem for Linux for the course or work on a computing cluster, such as Imperial's high performance computing cluster.</p> <p>The downstream analysis steps for RNA-seq data require less compute power and often need a more customised workflow. So, this is demonstrated separately in an R markdown notebook. You can run this notebook, located at <code>notebooks/downstream_analysis.Rmd</code>, or you can view a complete markdown version of the notebook in <code>docs/downstream_analysis.md</code>.</p>","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/rnaseq/#project-structure","title":"Project Structure","text":"<p>In this exemplar, we set up three versions of the same pipeline, that process RNA sequencing data. They are available in docs, along with the background information necessary to follow each pipeline. Each of these pipeline generates .count files in their final step, which counts how many RNA sequences map to each gene for each sample. We created multiple versions of the pipeline to show the different ways in which researchers might want to process their data. They are as follows:</p> <ol> <li>simple_pipeline: This is the simplest version of the pipeline. It is a bash script that can be run straight from the command line. It will execute each stage of the pipeline in sequentially.</li> </ol> <p>Pros:</p> <ul> <li>Simple to follow and execute</li> </ul> <p>Cons:</p> <ul> <li>slow for a large number of samples</li> <li> <p>if there is an error for a single sample the entire pipeline will fail</p> </li> <li> <p>parallelised_pipeline: This builds on the simple pipeline to run the pipeline on the cluster. It still uses bash scripts to orchestrate the pipeline, however it also allows each sample to be run in parallel on the cluster. This version of the pipeline is therefore a lot faster!</p> </li> </ul> <p>Pros:</p> <ul> <li>a lot faster than simple_pipeline</li> </ul> <p>Cons:</p> <ul> <li> <p>unwieldy to use for large numbers of samples</p> </li> <li> <p>nextflow_pipeline: This is the most advanced version of the pipeline. Instead of relying on bash scripts, that could fail without warning, it uses the nextflow workflow manager to run the pipeline stages either on your local computer or on a computing cluster.</p> </li> </ul> <p>Pros:</p> <ul> <li>automatically orchestrate the running of large numbers of samples</li> <li>run seamlessly both locally, on computing clusters and on cloud platforms</li> <li>easily shared with others</li> </ul> <p>Cons:</p> <ul> <li>takes more time to set up than the simple or parallelised pipeline</li> </ul> <p>We created markdown documents, available in this directory, for each of these pipelines. These documents explain the pipelines and link to external resources in case you want to learn more. We suggest you go through the documentation in the order above, as each one builds upon the last.</p> <p>The downstream analysis uses the output of these pipelines to investigate the data. The dataset is a lot smaller at this point, so this stage of the analysis can be accomplished locally without the use of the cluster. The code for this stage is contained within the notebooks/ directory. The code is written in an Rmarkdown document, which has been run and stored as a markdown document within this directory.</p>","tags":["HPC","Conda","Nextflow","Docker"]},{"location":"exemplars/scarceanomaly/","title":"Data-Scarce Anomaly Detection","text":"Redirecting to exemplar docs...","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#data-scarce-behavioural-anomaly-detection","title":"Data-Scarce Behavioural Anomaly Detection","text":"","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#overview","title":"Overview","text":"<p>This exemplar provides a complete pipeline for unsupervised anomaly detection applied to univariate time series data. Using the InternalBleeding14 dataset from the UCR Time Series Anomaly Archive, the project demonstrates techniques for detecting irregular patterns in physiological-style sensor recordings, where normal operating conditions are occasionally interrupted by anomalous deviations. The exemplar guides learners through data preparation, preprocessing, Isolation Forest modelling, dimensionality reduction with PCA, clustering with HDBSCAN, model interpretation, and ethical considerations when analysing scarce or sensitive time series data. The exemplar is fully modular, industry-aligned, and reproducible for academic and applied machine learning use cases.</p>","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#dataset","title":"Dataset","text":"<ul> <li>Dataset name: InternalBleeding14  </li> <li>Source: UCR Time Series Anomaly Archive  </li> <li>Citation:   \u25aa Wu, R., &amp; Keogh, E. (2020). Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress. arXiv:2009.13807</li> </ul>","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Week 1: Dataset loading, conversion and justification  </li> <li>Week 2: Preprocessing and baseline Isolation Forest  </li> <li>Week 3: PCA visualisation and HDBSCAN clustering  </li> <li>Week 4: Model interpretation with markdown explanations  </li> <li>Week 5: Ethical reflection on anomalies and data scarcity</li> <li>Week 6: Visual polishing and markdown refinement  </li> <li>Week 7: Environment file and reproducibility tests  </li> <li>Week 8: Final README, dataset framing, and documentation  </li> <li>Week 9: Review and minor corrections  </li> <li>Week 10: Final submission and packaging</li> </ul>","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#tools-used","title":"Tools Used","text":"<ul> <li>Python 3.x</li> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>HDBSCAN</li> <li>matplotlib</li> <li>seaborn</li> </ul>","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#repository-structure","title":"Repository Structure","text":"<pre><code>.\n\u251c\u2500\u2500 notebooks\n\u2502 \u251c\u2500\u2500 01_dataset_preparation.ipynb\n\u2502 \u251c\u2500\u2500 02_preprocessing_and_baseline_iforest.ipynb\n\u2502 \u251c\u2500\u2500 03_dimensionality_and_clustering.ipynb\n\u2502 \u251c\u2500\u2500 04_model_interpretation_and_explanation.ipynb\n\u2502 \u251c\u2500\u2500 05_ethical_reflection.ipynb\n\u2502 \u251c\u2500\u2500 06_visual_polishing_and_citations.ipynb\n\u2502 \u251c\u2500\u2500 07_reproducibility_and_environment_testing.ipynb\n\u2502 \u2514\u2500\u2500 08_finalised_summary_notebook.ipynb\n\u251c\u2500\u2500 data\n\u2502 \u2514\u2500\u2500 InternalBleeding14.csv\n\u251c\u2500\u2500 src\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 utils\n\u251c\u2500\u2500 test\n\u251c\u2500\u2500 LICENSE.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500 .github/workflows\n</code></pre>","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#licensing","title":"Licensing","text":"<p>BSD-3-Clause License</p>","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/scarceanomaly/#acknowledgements","title":"Acknowledgements","text":"<p>This exemplar was developed at Imperial College London by Duke T. J. Ludera, in collaboration with Saranjeet Kaur S. S. Bhogal from Research Software Engineering, and Dr Jianliang Gao from Research Computing &amp; Data Science at the Early Career Researcher Institute.</p>","tags":["Behavioural Anomaly Detection","Unsupervised Learning","Data Scarcity","AI Ethics","Pandas","Scikit Learn"]},{"location":"exemplars/sphnavierstokes/","title":"SPH Solver for 2D Navier-Stokes","text":"Redirecting to exemplar docs...","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#recode-sph-solver-2d-ns","title":"RECODE-SPH-SOLVER-2D-NS","text":"SPH simulation","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#description","title":"Description","text":"<p>In this project we present a numerical code in C++ which solves the two-dimensional Navier-Stokes equations using the smoothed-particle hydrodynamics (SPH) approach. The focus lies in the implementation (and documentation) of good C++ practices and the development of skills related to efficient, robust, extensible and readable scientific code. The learning process regarding this project can be twofold:</p> <p>1) The student can study the material provided in the <code>main</code> branch of the present repository. It is independent of all the other branches and can be used as a standalone educational resource. In this the implemented SPH methodology is explained as well as the structure of the source code and the post-processing scripts.</p> <p>2) The student can start by studying progressively the branches <code>v0</code> - <code>v5</code> in order to experience the process which was followed in order to improve and optimize the herein code. Several comments have been added for each individual version in the corresponding branch to highlight the improvements which were implemented compared to its ancestors.</p>","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>I/O (input/output)</li> <li>OOP (object-oriented programming)</li> <li>C++ containers</li> <li>Performance and memory optimization tools and skills</li> </ul> Task Time Reading 15 hours Practicing 10 hours","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#requirements","title":"Requirements","text":"","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#academic","title":"Academic","text":"<ul> <li> <p>Experience with basic programming concepts (for loops, functions, reading and writing files etc.).</p> </li> <li> <p>Some experience with C++ (familiarity with pointers, C++ classes and the use of external libraries).</p> </li> <li> <p>Basic understanding of numerical analysis concepts (time marching, temporal integration etc.)</p> </li> </ul>","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#system","title":"System","text":"<p>For manual installation of the program and its dependencies, you will need the following:</p> <ul> <li>Python 3.11 installed</li> <li>A C++ toolchain along with the necessary development libraries: <ul> <li>build-essential </li> <li>libboost-program-options-dev, </li> <li>clang-format</li> <li>cmake. </li> </ul> </li> </ul> <p>You will also need to install the required Python packages by running the command </p> <ul> <li>\"pip install -r requirements.txt.\"</li> </ul> <p>If you haven't already, set up the pre-commit hook by executing the </p> <ul> <li>\"pre-commit install\" command.</li> </ul> <p>Following these steps manually will establish the environment necessary for the program to run successfully. Alternatively, to streamline the process, consider using the provided Dockerfile to create a Docker image and deploy the program in a containerized environment, or you can simply use the github codespaces functionality by following the instructions provided in the section GitHub Codespaces.</p>","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#getting-started","title":"Getting Started","text":"<p>To better comprehend this exemplar, it's recommended to follow the outlined procedure by sequentially reviewing the chapters. The suggested workflow ensures a step-by-step understanding, beginning with the foundational SPH algorithm elucidated in the code (Smoothed-Particle Hydrodynamics (SPH)-Adaptive timestep), followed by an exploration of its implementation aspects (Code overview - Efficient programming). Once you've grasped the code intricacies, it's time to put it into action. In the sections Code execution - Profiling you will learn how to run the code, assess its performance and post process the data it generates. Depending on your learning preference, you may opt for a reversed approach, initiating with code execution and subsequently delving under the hood to explore the C++ code. It works both ways!</p> <p>For additional study and practice, delve into Clang format, Building with Cmake and Exercises. These sections offer deeper insights into the project's good practices and provide opportunities to enhance your coding skills.</p>","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 1.SPH.md\n\u2502   \u251c\u2500\u2500 2.Code_Overview.md\n\u2502   \u251c\u2500\u2500 3.IO_Overview.md\n\u2502   \u251c\u2500\u2500 4.OOP_Concepts.md\n\u2502   \u251c\u2500\u2500 5.STL.md\n\u2502   \u251c\u2500\u2500 6.Efficient_Programming.md\n\u2502   \u251c\u2500\u2500 7.Neighbour_Search.md\n\u2502   \u251c\u2500\u2500 8.Adaptive_Timestep.md\n\u2502   \u251c\u2500\u2500 9.Code_Execution.md\n\u2502   \u251c\u2500\u2500 10.GH_Codespaces.md\n\u2502   \u251c\u2500\u2500 11.Profiling.md\n\u2502   \u251c\u2500\u2500 12.Post_Analysis.md\n\u2502   \u251c\u2500\u2500 A1.ClangFormat.md\n\u2502   \u251c\u2500\u2500 A2.CMake.md\n\u2502   \u2514\u2500\u2500 B1.Exercises.md\n\u251c\u2500\u2500 exec\n\u2502   \u251c\u2500\u2500 build\n\u2502   \u2514\u2500\u2500 input\n\u2502       \u251c\u2500\u2500 case.txt\n\u2502       \u251c\u2500\u2500 constants.txt\n\u2502       \u251c\u2500\u2500 domain.txt\n\u2502       \u251c\u2500\u2500 ic-block-drop.txt\n\u2502       \u251c\u2500\u2500 ic-droplet.txt\n\u2502       \u251c\u2500\u2500 ic-one-particles.txt\n\u2502       \u251c\u2500\u2500 ic-two-particles.txt\n\u2502       \u251c\u2500\u2500 ic-three-particles.txt\n\u2502       \u2514\u2500\u2500 ic-four-particles.txt\n\u251c\u2500\u2500 notebooks\n\u251c\u2500\u2500 post\n\u2502   \u251c\u2500\u2500 plot_energies.ipynb\n\u2502   \u251c\u2500\u2500 plot_energies.py\n\u2502   \u251c\u2500\u2500 simulation_animation.ipynb\n\u2502   \u251c\u2500\u2500 simulation_animation.py\n\u2502   \u251c\u2500\u2500 visualise_particles.ipynb\n\u2502   \u2514\u2500\u2500 visualise_particles.py\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 CMakeLists.txt\n    \u251c\u2500\u2500 fluid.cpp\n    \u251c\u2500\u2500 fluid.h\n    \u251c\u2500\u2500 initial_conditions.cpp\n    \u251c\u2500\u2500 initial_conditions.h\n    \u251c\u2500\u2500 main_prog_funcs.h\n    \u251c\u2500\u2500 particles.cpp\n    \u251c\u2500\u2500 particles.h\n    \u251c\u2500\u2500 SPH-main.cpp\n    \u251c\u2500\u2500 sph_solver.cpp\n    \u2514\u2500\u2500 sph_solver.h\n</code></pre> <sub>Chris Cooling</sub> <sub>Christos Petalotis</sub> <sub>Katerina Michalickova</sub> <sub>Dan Cummins</sub> <sub>Vyron Avramidis</sub> <sub>Vasileios Christou</sub> <sub>George Efstathiou</sub> <sub>George Efstathiou</sub>","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/sphnavierstokes/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Boost","Fluid Dynamics","Optimisation","Object-Oriented Programming","Profiling","Visualisation","Matplotlib","CMake"]},{"location":"exemplars/stockpopulation/","title":"Stochastic Population Dynamics in Changing Environments","text":"Redirecting to exemplar docs...","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#cell-population-dynamics-in-continuous-time-domain","title":"Cell Population Dynamics in Continuous Time Domain","text":"","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#description","title":"Description","text":"<p>This project explores the mathematical modeling of cell population dynamics using the McKendrick\u2013Von Foerster equation. The focus is on understanding the time-independent case where cell division and death rates depend on the cell's age.</p>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Understand the McKendrick\u2013Von Foerster equation for population dynamics.</li> <li>Apply separation of variables to solve partial differential equations.</li> <li>Derive and interpret the Euler-Lotka equation for population growth rates.</li> </ul>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#requirements","title":"Requirements","text":"","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#academic","title":"Academic","text":"<ul> <li>Basic knowledge of differential equations and mathematical modeling.</li> <li>Familiarity with population dynamics concepts.</li> </ul>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#system","title":"System","text":"<ul> <li>Python 3.11 or newer</li> <li>MkDocs for documentation generation</li> <li>Miniconda (recommended for managing dependencies)</li> </ul>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#getting-started","title":"Getting Started","text":"<ol> <li>Clone the repository and navigate to the project directory.</li> <li>Set up a virtual environment and install the required dependencies:    <pre><code>conda create --name cell-population-dynamics python=3.11\nconda activate cell-population-dynamics\npip install -r requirements.txt\n</code></pre></li> </ol>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 notebooks\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 __init__.py\n|   \u251c\u2500\u2500 cells_manager.py   # Cell population manager\n|   \u251c\u2500\u2500 plots.py           # Plotting functions\n|   \u251c\u2500\u2500 run.py             # Main script to run the simulation\n|   \u251c\u2500\u2500 simulation.py      # Core simulation logic\n\u2502   \u2514\u2500\u2500 utils.py           # Utility functions\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 README.md\n</code></pre>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#mkdocs-documentation","title":"MkDocs Documentation","text":"<p>To generate local documentation, run the following command:</p> <pre><code>mkdocs serve\n</code></pre>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/stockpopulation/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Stochastic Differential Equations","Ordinary Differential Equations"]},{"location":"exemplars/turingpatterns/","title":"Turing Patterns & Partial Differential Equations","text":"Redirecting to exemplar docs...","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#turing-patterns-and-partial-differential-equations","title":"Turing Patterns and Partial Differential Equations","text":"","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#description","title":"Description","text":"<p>This code is a component of the Research Computing and Data Science Examples (ReCoDE) project. It comprises a non-linear partial differential equation (PDE) solver implemented in Fortran, designed to address both boundary value problems (BVP) and initial boundary value problems (IBVP) with temporal progression. The solver's versatility allows it to handle problems in one or two dimensions and can accommodate single equations or pairs of coupled equations. This exemplar showcases several key features of the code, including:</p> <ul> <li>Integration with the Fortran Package Manager (FPM)</li> <li>Utilization of LAPACK libraries</li> <li>Modular architecture for enhanced maintainability and extensibility</li> </ul> <p>To demonstrate its practical application, the code solves a PDE derived from a predator-prey model. This model is renowned for generating solutions that exhibit Turing patterns, which are observed in various biological systems, such as the skin patterns of pufferfish. While prior knowledge of PDEs and predator-prey models is not prerequisite for understanding this example, all relevant mathematical concepts will be elucidated in the subsequent sections.</p>","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Complied Codes</li> <li>Fortran Package Manager (FPM)</li> <li>Modular Codes</li> <li>Multipurpose Codes</li> <li>Solving Mathematical Problems (PDEs)</li> <li>Generalising Problems</li> <li>Discretisation in multiple dimensions</li> <li>Use of external libraries (LAPACK and BLAS)</li> <li>Testing Fortran code</li> </ul> Task Time Reading 7 hours Practising 7 hours","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#requirements","title":"Requirements","text":"<p>This exemplar is for entry-level researchers with basic knowledge of Fortran syntax. An Introduction to Fortran Course  is available to help you get started.</p>","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#academic","title":"Academic","text":"<ul> <li>Ordinary Differential Equations (ODEs): Ordinary differential equations are equations that involve functions of a single independent variable and their derivatives. These equations are fundamental in modeling various physical, biological, and economic phenomena where the rate of change of a quantity is related to the quantity itself. A discussion on ordinary differential equations can be found here</li> <li>Partial Differential Equations (PDEs): Partial differential equations are more complex than ODEs, as they involve functions of multiple independent variables and their partial derivatives. PDEs are crucial in describing many natural phenomena, including heat transfer, fluid dynamics, and quantum mechanics. They allow us to model systems that change with respect to multiple variables simultaneously, such as time and space. A discussion on partial differential equations can be found here.</li> <li>Numerical Techniques: While some differential equations can be solved analytically, many real-world problems require numerical methods for approximation. The numerical techniques used for solving PDEs are often similar to those used for ODEs, but they must account for the additional complexity introduced by multiple variables.</li> <li>Finite Difference Method: One common numerical approach for solving PDEs is the finite difference method. This technique approximates derivatives by differences over small intervals. It discretises the continuous domain of the PDE into a grid or mesh, and the solution is computed at discrete points. This method is particularly useful for problems with regular geometries and is relatively straightforward to implement.</li> </ul> <p>Understanding these concepts is crucial for working with the non-linear PDE solver included in this exemplar. The solver's ability to handle both BVPs and IVBPs makes it a versatile tool for a wide range of applications in scientific computing and mathematical modeling.</p>","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#system","title":"System","text":"<ul> <li>A Fortran compiler, such as <code>gfortran</code>: see here for an installation guide.</li> <li>Fortran Package Manager (FPM): see here for an installation guide.</li> <li><code>BLAS</code> external library: see here for a <code>BLAS</code> installation guide here. Mac users can install with homebrew. </li> <li><code>LAPACK</code> external library: see the <code>LAPACK</code> documentation here and here for an installation guide. Mac users can install with homebrew. <code>BLAS</code> must be installed first.</li> <li>Optional: For visualisation of solutions we have made use of <code>MATLAB_R2023a</code> code. These are given in the <code>solver/examples</code> directory. Other visualisation software can be used.</li> </ul>","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#getting-started","title":"Getting Started","text":"<p>To begin working with this PDE solver, please follow these steps:</p> <ul> <li> <p>Introduction to Fortran Package Manager (FPM): Start by reading the review of Fortran Package Manager in Section 00 which is required for compiling this project. This will provide an introduction to FPM using the <code>fortran_fibonacci</code> repository as an example.</p> </li> <li> <p>Theoretical Background: Section 01 provides a general overview of the problem and the basic techniques employed in the solution.</p> </li> <li> <p>Solver Configuration and Usage: The three parts of Section 03 describes how to run and use the code.</p> </li> <li> <p>Example Problems: Consult Section 9 for detailed instructions on solving specific problems using this code.</p> </li> <li> <p>Advanced Techniques: The remaining sections explain specific coding techniques used in the development of this solver.</p> </li> </ul> <p>This structured learning sequence provides a comprehensive overview of the PDE solver, encompassing its theoretical foundations, practical implementation, operational use, and critically, the coding techniques employed in its development.</p>","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]},{"location":"exemplars/turingpatterns/#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>","tags":["Fortran Package Manager","Partial Differential Equations","LAPACK","Finite Difference Method","Unit Testing"]}]}